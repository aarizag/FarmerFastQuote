Information theory studies the transmission, processing, extraction, and utilization of information. Abstractly, information can be thought of as the resolution of uncertainty. In the case of communication of information over a noisy channel, this abstract concept was made concrete in 1948 by Claude Shannon in his paper "A Mathematical Theory of Communication", in which "information" is thought of as a set of possible messages, where the goal is to send these messages over a noisy channel, and then to have the receiver reconstruct the message with low probability of error, in spite of the channel noise. Shannon's main result, the noisy-channel coding theorem showed that, in the limit of many channel uses, the rate of information that is asymptotically achievable is equal to the channel capacity, a quantity dependent merely on the statistics of the channel over which the messages are sent.[1]

Information theory is closely associated with a collection of pure and applied disciplines that have been investigated and reduced to engineering practice under a variety of rubrics throughout the world over the past half century or more: adaptive systems, anticipatory systems, artificial intelligence, complex systems, complexity science, cybernetics, informatics, machine learning, along with systems sciences of many descriptions. Information theory is a broad and deep mathematical theory, with equally broad and deep applications, amongst which is the vital field of coding theory.

Coding theory is concerned with finding explicit methods, called codes, for increasing the efficiency and reducing the error rate of data communication over noisy channels to near the channel capacity. These codes can be roughly subdivided into data compression (source coding) and error-correction (channel coding) techniques. In the latter case, it took many years to find the methods Shannon's work proved were possible. A third class of information theory codes are cryptographic algorithms (both codes and ciphers). Concepts, methods and results from coding theory and information theory are widely used in cryptography and cryptanalysis. See the article ban (unit) for a historical application.

Information theory is also used in information retrieval, intelligence gathering, gambling, statistics, and even in musical composition.

The landmark event that established the discipline of information theory and brought it to immediate worldwide attention was the publication of Claude E. Shannon's classic paper "A Mathematical Theory of Communication" in the Bell System Technical Journal in July and October 1948.

Prior to this paper, limited information-theoretic ideas had been developed at Bell Labs, all implicitly assuming events of equal probability. Harry Nyquist's 1924 paper, Certain Factors Affecting Telegraph Speed, contains a theoretical section quantifying "intelligence" and the "line speed" at which it can be transmitted by a communication system, giving the relation W = K log m (recalling Boltzmann's constant), where W is the speed of transmission of intelligence, m is the number of different voltage levels to choose from at each time step, and K is a constant. Ralph Hartley's 1928 paper, Transmission of Information, uses the word information as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as H = log Sn = n log S, where S was the number of possible symbols, and n the number of symbols in a transmission. The unit of information was therefore the decimal digit, which has since sometimes been called the hartley in his honor as a unit or scale or measure of information. Alan Turing in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers.

Much of the mathematics behind information theory with events of different probabilities were developed for the field of thermodynamics by Ludwig Boltzmann and J. Willard Gibbs. Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by Rolf Landauer in the 1960s, are explored in Entropy in thermodynamics and information theory.

In Shannon's revolutionary and groundbreaking paper, the work for which had been substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion that

"The fundamental problem of communication is that of reproducing at one point, either exactly or approximately, a message selected at another point."

Information theoretic concepts apply to cryptography and cryptanalysis. Turing's information unit, the ban, was used in the Ultra project, breaking the German Enigma machine code and hastening the end of World War II in Europe. Shannon himself defined an important concept now called the unicity distance. Based on the redundancy of the plaintext, it attempts to give a minimum amount of ciphertext necessary to ensure unique decipherability.

Information theory leads us to believe it is much more difficult to keep secrets than it might first appear. A brute force attack can break systems based on asymmetric key algorithms or on most commonly used methods of symmetric key algorithms (sometimes called secret key algorithms), such as block ciphers. The security of all such methods currently comes from the assumption that no known attack can break them in a practical amount of time.

Information theoretic security refers to methods such as the one-time pad that are not vulnerable to such brute force attacks. In such cases, the positive conditional mutual information between the plaintext and ciphertext (conditioned on the key) can ensure proper transmission, while the unconditional mutual information between the plaintext and ciphertext remains zero, resulting in absolutely secure communications. In other words, an eavesdropper would not be able to improve his or her guess of the plaintext by gaining knowledge of the ciphertext but not of the key. However, as in any other cryptographic system, care must be used to correctly apply even information-theoretically secure methods; the Venona project was able to crack the one-time pads of the Soviet Union due to their improper reuse of key material.


Decision theory
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Decision theory (or the theory of choice) is the study of the reasoning underlying an agent's choices.[1] Decision theory can be broken into three branches: normative decision theory, which gives advice on how to make the best decisions, given a set of uncertain beliefs and a set of values; descriptive decision theory, which analyzes how existing, possibly irrational agents actually make decisions; and prescriptive decision theory, which tries to guide or give procedures on how or what we should do in order to make best decisions in line with the normative theory.

Closely related to the field of game theory,[2] decision theory is concerned with the choices of individual agents whereas game theory is concerned with interactions of agents whose decisions affect each other. Decision theory is an interdisciplinary topic, studied by economists, statisticians, psychologists, biologists,[3] political and other social scientists, philosophers,[4] and computer scientists.

Empirical applications of this rich theory are usually done with the help of statistical and econometric methods, especially via the so-called choice models, such as probit and logit models. Estimation of such models is usually done via parametric, semi-parametric and non-parametric maximum likelihood methods.[5]


Contents
1	Normative and descriptive
2	What kinds of decisions need a theory?
2.1	Choice under uncertainty
2.2	Intertemporal choice
2.3	Interaction of decision makers
2.4	Complex decisions
3	Heuristics
4	Alternatives
4.1	Probability theory
4.2	Alternatives to probability theory
4.3	Ludic fallacy
5	See also
6	References
7	Further reading
Normative and descriptive
Normative decision theory is concerned with identifying the best decision to make, modelling an ideal decision maker who is able to compute with perfect accuracy and is fully rational. The practical application of this prescriptive approach (how people ought to make decisions) is called decision analysis, and is aimed at finding tools, methodologies and software (decision support systems) to help people make better decisions.

In contrast, positive or descriptive decision theory is concerned with describing observed behaviors under the assumption that the decision-making agents are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomatic framework, reconciling the Von Neumann-Morgenstern axioms with behavioral violations of the expected utility hypothesis, or they may explicitly give a functional form for time-inconsistent utility functions (e.g. Laibson's quasi-hyperbolic discounting).

The prescriptions or predictions about behaviour that positive decision theory produces allow for further tests of the kind of decision-making that occurs in practice. There is a thriving dialogue with experimental economics, which uses laboratory and field experiments to evaluate and inform theory. In recent decades, there has also been increasing interest in what is sometimes called "behavioral decision theory" and this has contributed to a re-evaluation of what rational decision-making requires.[6]

What kinds of decisions need a theory?
Choice under uncertainty
Further information: Expected utility hypothesis
The area of choice under uncertainty represents the heart of decision theory. Known from the 17th century (Blaise Pascal invoked it in his famous wager, which is contained in his Pensées, published in 1670), the idea of expected value is that, when faced with a number of actions, each of which could give rise to more than one possible outcome with different probabilities, the rational procedure is to identify all possible outcomes, determine their values (positive or negative) and the probabilities that will result from each course of action, and multiply the two to give an "expected value", or the average expectation for an outcome; the action to be chosen should be the one that gives rise to the highest total expected value. In 1738, Daniel Bernoulli published an influential paper entitled Exposition of a New Theory on the Measurement of Risk, in which he uses the St. Petersburg paradox to show that expected value theory must be normatively wrong. He gives an example in which a Dutch merchant is trying to decide whether to insure a cargo being sent from Amsterdam to St Petersburg in winter. In his solution, he defines a utility function and computes expected utility rather than expected financial value (see[7] for a review).

In the 20th century, interest was reignited by Abraham Wald's 1939 paper[8] pointing out that the two central procedures of sampling-distribution-based statistical-theory, namely hypothesis testing and parameter estimation, are special cases of the general decision problem. Wald's paper renewed and synthesized many concepts of statistical theory, including loss functions, risk functions, admissible decision rules, antecedent distributions, Bayesian procedures, and minimax procedures. The phrase "decision theory" itself was used in 1950 by E. L. Lehmann.[9]

The revival of subjective probability theory, from the work of Frank Ramsey, Bruno de Finetti, Leonard Savage and others, extended the scope of expected utility theory to situations where subjective probabilities can be used. At the time, von Neumann and Morgenstern theory of expected utility[10] proved that expected utility maximization followed from basic postulates about rational behavior.

The work of Maurice Allais and Daniel Ellsberg showed that human behavior has systematic and sometimes important departures from expected-utility maximization. The prospect theory of Daniel Kahneman and Amos Tversky renewed the empirical study of economic behavior with less emphasis on rationality presuppositions. Kahneman and Tversky found three regularities – in actual human decision-making, "losses loom larger than gains"; persons focus more on changes in their utility-states than they focus on absolute utilities; and the estimation of subjective probabilities is severely biased by anchoring.

Intertemporal choice
Main article: Intertemporal choice
Intertemporal choice is concerned with the kind of choice where different actions lead to outcomes that are realised at different points in time. If someone received a windfall of several thousand dollars, they could spend it on an expensive holiday, giving them immediate pleasure, or they could invest it in a pension scheme, giving them an income at some time in the future. What is the optimal thing to do? The answer depends partly on factors such as the expected rates of interest and inflation, the person's life expectancy, and their confidence in the pensions industry. However even with all those factors taken into account, human behavior again deviates greatly from the predictions of prescriptive decision theory, leading to alternative models in which, for example, objective interest rates are replaced by subjective discount rates.

Interaction of decision makers
Some decisions are difficult because of the need to take into account how other people in the situation will respond to the decision that is taken. The analysis of such social decisions is more often treated under the label of game theory, rather than decision theory, though it involves the same mathematical methods. From the standpoint of game theory most of the problems treated in decision theory are one-player games (or the one player is viewed as playing against an impersonal background situation). In the emerging socio-cognitive engineering, the research is especially focused on the different types of distributed decision-making in human organizations, in normal and abnormal/emergency/crisis situations.[11]

Complex decisions
Other areas of decision theory are concerned with decisions that are difficult simply because of their complexity, or the complexity of the organization that has to make them. Individuals making decisions may be limited in resources or are boundedly rational (have finite time or intelligence); in such cases the issue, more than the deviation between real and optimal behaviour, is the difficulty of determining the optimal behaviour in the first place. One example is the model of economic growth and resource usage developed by the Club of Rome to help politicians make real-life decisions in complex situations[citation needed]. Decisions are also affected by whether options are framed together or separately; this is known as the distinction bias. In 2011, Dwayne Rosenburgh explored and showed how decision theory can be applied to complex decisions that arise in areas such as wireless communications.[12]

Heuristics
Main article: Heuristic
The heuristic approach to decision-making makes decisions based on routine thinking, which, while quicker than step-by-step processing, opens the risk of introducing inaccuracies, mistakes and fallacies, which may be easily disproved in a step-by-step process of thinking.[13] One example of common and incorrect thought process is the gambler's fallacy, or believing that a random event is affected by previous random events (truth is, there is a fifty percent chance of a coin landing on heads even after a long sequence of tails). Another example is that decision-makers may be biased towards preferring moderate alternatives to extreme ones; the "Compromise Effect" operates under a mindset driven by the belief that the most moderate option, amid extremes, carries the most benefits from each extreme.[14]

Alternatives
A highly controversial issue is whether one can replace the use of probability in decision theory by other alternatives.

Probability theory
Advocates for the use of probability theory point to:

the work of Richard Threlkeld Cox for justification of the probability axioms,
the Dutch book paradoxes of Bruno de Finetti as illustrative of the theoretical difficulties that can arise from departures from the probability axioms, and
the complete class theorems, which show that all admissible decision rules are equivalent to the Bayesian decision rule for some utility function and some prior distribution (or for the limit of a sequence of prior distributions). Thus, for every decision rule, either the rule may be reformulated as a Bayesian procedure (or a limit of a sequence of such), or there is a rule that is sometimes better and never worse.
Alternatives to probability theory
The proponents of fuzzy logic, possibility theory, quantum cognition, Dempster–Shafer theory, and info-gap decision theory maintain that probability is only one of many alternatives and point to many examples where non-standard alternatives have been implemented with apparent success; notably, probabilistic decision theory is sensitive to assumptions about the probabilities of various events, while non-probabilistic rules such as minimax are robust, in that they do not make such assumptions.

Ludic fallacy
Main article: Ludic fallacy
A general criticism of decision theory based on a fixed universe of possibilities is that it considers the "known unknowns", not the "unknown unknowns"[citation needed]: it focuses on expected variations, not on unforeseen events, which some argue (as in black swan theory) have outsized impact and must be considered – significant events may be "outside model". This line of argument, called the ludic fallacy, is that there are inevitable imperfections in modeling the real world by particular models, and that unquestioning reliance on models blinds one to their limits.

See also
	Wikiquote has quotations related to: Decision theory
Bayesian statistics
Causal decision theory
Choice modelling
Constraint satisfaction
Decision making
Evidential decision theory
Game theory
Multi-criteria decision making
Operations research
Optimal decision
Decision quality
Preference (economics)
Quantum cognition
Rationality
Secretary problem
Signal detection theory
Small-numbers game
Stochastic dominance
TOTREP
Two envelopes problem
References
 Steele, Katie and Stefánsson, H. Orri, "Decision Theory", The Stanford Encyclopedia of Philosophy (Winter 2015 Edition), Edward N. Zalta (ed.), URL = [1]
 Myerson, Roger B. (1991). "1.2: Basic concepts of Decision Theory". Game theory analysis of conflict. Cambridge, Massachusetts: Harvard University Press. ISBN 9780674728615.
 Habibi, Iman; Cheong, Raymond; Lipniacki, Tomasz; Levchenko, Andre; Emamian, Effat S.; Abdi, Ali (2017-04-05). "Computation and measurement of cell decision making errors using single cell data". PLOS Computational Biology. 13 (4): e1005436. doi:10.1371/journal.pcbi.1005436. ISSN 1553-7358.
 Hansson, Sven Ove. "Decision theory: A brief introduction." (2005) Section 1.2: A truly interdisciplinary subject.
 Park, Byeong U.; Simar, Léopold; Zelenyuk, Valentin (2017). "Nonparametric estimation of dynamic discrete choice models for time series data". Computational Statistics & Data Analysis. 108: 97–120. doi:10.1016/j.csda.2016.10.024.
 For instance, see: Anand, Paul (1993). Foundations of Rational Choice Under Risk. Oxford: Oxford University Press. ISBN 0-19-823303-5.
 Schoemaker, P. J. H. (1982). "The Expected Utility Model: Its Variants, Purposes, Evidence and Limitations". Journal of Economic Literature. 20: 529–563.
 Wald, Abraham (1939). "Contributions to the Theory of Statistical Estimation and Testing Hypotheses". Annals of Mathematical Statistics. 10 (4): 299–326. doi:10.1214/aoms/1177732144. MR 0000932.
 Lehmann, E. L. (1950). "Some Principles of the Theory of Testing Hypotheses". Annals of Mathematical Statistics. 21 (1): 1–26. doi:10.1214/aoms/1177729884. JSTOR 2236552.
 Neumann, John von; Morgenstern, Oskar (1953) [1944]. Theory of Games and Economic Behavior (Third ed.). Princeton, NJ: Princeton University Press.
 Crozier, M. & Friedberg, E. 1995. "Organization and Collective Action. Our Contribution to Organizational Analysis" in Bacharach S.B, Gagliardi P. & Mundell P. (Eds). Research in the Sociology of Organizations. Vol. XIII, Special Issue on European Perspectives of Organizational Theory, Greenwich, CT: JAI Press.
 Rosenburgh, D. 2011. "Decision Theory with its Applications in Wireless Communication" in Zhang, Y. (Ed.), GUIZANI, M. (Ed.). (2011). Game Theory for Wireless Communications and Networking. Boca Raton: CRC Press.
 Johnson, E. J.; Payne, J. W. (1985). "EFFORT AND ACCURACY IN CHOICE". Management Science. 31 (4): 395–414. doi:10.1287/mnsc.31.4.395.
 Roe, R. M.; Busemeyer, J. R.; Townsend, J. T. (2001). "Multialternative decision field theory: A dynamic connectionist model of decision making". Psychological Review. 108 (2): 370–392. doi:10.1037/0033-295X.108.2.370.
Further reading
Akerlof, George A.; Yellen, Janet L. (May 1987). "Rational Models of Irrational Behavior". 77 (2): 137–142.
Anand, Paul (1993). Foundations of Rational Choice Under Risk. Oxford: Oxford University Press. ISBN 0-19-823303-5. (an overview of the philosophical foundations of key mathematical axioms in subjective expected utility theory – mainly normative)
Arthur, W. Brian (May 1991). "Designing Economic Agents that Act like Human Agents: A Behavioral Approach to Bounded Rationality". The American Economic Review. 81 (2): 353–9.
Berger, James O. (1985). Statistical decision theory and Bayesian Analysis (2nd ed.). New York: Springer-Verlag. ISBN 0-387-96098-8. MR 0804611.
Bernardo, José M.; Smith, Adrian F. M. (1994). Bayesian Theory. Wiley. ISBN 0-471-92416-4. MR 1274699.
Clemen, Robert; Reilly, Terence (2014). Making Hard Decisions with DecisionTools: An Introduction to Decision Analysis (3rd ed.). Stamford CT: Cengage. ISBN 0-538-79757-6. (covers normative decision theory)
De Groot, Morris, Optimal Statistical Decisions. Wiley Classics Library. 2004. (Originally published 1970.) ISBN 0-471-68029-X.
Goodwin, Paul; Wright, George (2004). Decision Analysis for Management Judgment (3rd ed.). Chichester: Wiley. ISBN 0-470-86108-8. (covers both normative and descriptive theory)
Hansson, Sven Ove. "Decision Theory: A Brief Introduction" (PDF). Archived from the original (PDF) on July 5, 2006.
Khemani, Karan, Ignorance is Bliss: A study on how and why humans depend on recognition heuristics in social relationships, the equity markets and the brand market-place, thereby making successful decisions, 2005.
Leach, Patrick (2006). Why Can't You Just Give Me the Number? An Executive's Guide to Using Probabilistic Thinking to Manage Risk and to Make Better Decisions. Probabilistic. ISBN 0-9647938-5-7. A rational presentation of probabilistic analysis.
Miller L (1985). "Cognitive risk-taking after frontal or temporal lobectomy—I. The synthesis of fragmented visual information". Neuropsychologia. 23 (3): 359–69. doi:10.1016/0028-3932(85)90022-3. PMID 4022303.
Miller L, Milner B (1985). "Cognitive risk-taking after frontal or temporal lobectomy—II. The synthesis of phonemic and semantic information". Neuropsychologia. 23 (3): 371–9. doi:10.1016/0028-3932(85)90023-5. PMID 4022304.
North, D.W. (1968). "A tutorial introduction to decision theory". IEEE Transactions on Systems Science and Cybernetics. 4 (3): 200–210. doi:10.1109/TSSC.1968.300114. Reprinted in Shafer & Pearl. (also about normative decision theory)
Peterson, Martin (2009). An Introduction to Decision Theory. Cambridge University Press. ISBN 978-0-521-71654-3.
Raiffa, Howard (1997). Decision Analysis: Introductory Lectures on Choices Under Uncertainty. McGraw Hill. ISBN 0-07-052579-X.
Robert, Christian (2007). The Bayesian Choice (2nd ed.). New York: Springer. doi:10.1007/0-387-71599-1. ISBN 0-387-95231-4. MR 1835885.
Shafer, Glenn; Pearl, Judea, eds. (1990). Readings in uncertain reasoning. San Mateo, CA: Morgan Kaufmann.
Smith, J.Q. (1988). Decision Analysis: A Bayesian Approach. Chapman and Hall. ISBN 0-412-27520-1.
Charles Sanders Peirce and Joseph Jastrow (1885). "On Small Differences in Sensation". Memoirs of the National Academy of Sciences. 3: 73–83. http://psychclassics.yorku.ca/Peirce/small-diffs.htm
Ramsey, Frank Plumpton; "Truth and Probability" (PDF), Chapter VII in The Foundations of Mathematics and other Logical Essays (1931).
de Finetti, Bruno (September 1989). "Probabilism: A Critical Essay on the Theory of Probability and on the Value of Science". Erkenntnis. 31. (translation of 1931 article)
de Finetti, Bruno (1937). "La Prévision: ses lois logiques, ses sources subjectives". Annales de l'Institut Henri Poincaré.
de Finetti, Bruno. "Foresight: its Logical Laws, Its Subjective Sources," (translation of the 1937 article in French) in H. E. Kyburg and H. E. Smokler (eds), Studies in Subjective Probability, New York: Wiley, 1964.
de Finetti, Bruno. Theory of Probability, (translation by AFM Smith of 1970 book) 2 volumes, New York: Wiley, 1974-5.
Donald Davidson, Patrick Suppes and Sidney Siegel (1957). Decision-Making: An Experimental Approach. Stanford University Press.
Pfanzagl, J (1967). "Morgenstern". In Martin Shubik. Essays in Mathematical Economics In Honor of Oskar Morgenstern. Princeton University Press. pp. 237–251.
Pfanzagl, J. in cooperation with V. Baumann and H. Huber (1968). "Events, Utility and Subjective Probability". Theory of Measurement. Wiley. pp. 195–220.
Morgenstern, Oskar (1976). "Some Reflections on Utility". In Andrew Schotter. Selected Economic Writings of Oskar Morgenstern. New York University Press. pp. 65–70. ISBN 0-8147-7771-6.
Non-Robust Models in Statistics by Lev B. Klebanov, Svetlozat T. Rachev and Frank J. Fabozzi, Nova Scientific Publishers, Inc. New York, 2009.

vte
Paradoxes
vte
Industrial and applied mathematics
vte
Subfields of and scientists involved in cybernetics
vte
Economics
Authority control Edit this at Wikidata	
GND: 4138606-1
Categories: Decision theoryStatistical inferenceEconomics of uncertaintyRiskControl theoryFormal sciencesEpistemology of scienceMathematical and quantitative methods (economics)
Navigation menu
Not logged inTalkContributionsCreate accountLog inArticleTalkReadEditView historySearch

Search Wikipedia
Main page
Contents
Featured content
Current events
Random article
Donate to Wikipedia
Wikipedia store
Interaction
Help
About Wikipedia
Community portal
Recent changes
Contact page
Tools
What links here
Related changes
Upload file
Special pages
Permanent link
Page information
Wikidata item
Cite this page
Print/export
Create a book
Download as PDF
Printable version
In other projects
Wikimedia Commons
Wikiquote

Languages
???????
Deutsch
Español
Français
???
Bahasa Indonesia
???????
Ti?ng Vi?t
??
17 more
Edit links
This page was last edited on 1 August 2018, at 17:59 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaDevelopersCookie statementMobile viewWikimedia Foundation Powered by MediaWiki


Information theory
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Not to be confused with information science.
Information theory studies the quantification, storage, and communication of information. It was originally proposed by Claude E. Shannon in 1948 to find fundamental limits on signal processing and communication operations such as data compression, in a landmark paper entitled "A Mathematical Theory of Communication". Applications of fundamental topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for digital subscriber line (DSL)). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields.

A key measure in information theory is "entropy". Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy.

The field is at the intersection of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering. The theory has also found applications in other areas, including statistical inference, natural language processing, cryptography, neurobiology[1], human vision[2] , the evolution[3] and function[4] of molecular codes (bioinformatics), model selection in statistics,[5] thermal physics,[6] quantum computing, linguistics, plagiarism detection,[7] pattern recognition, and anomaly detection.[8] Important sub-fields of information theory include source coding, channel coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, and measures of information.


Contents
1	Overview
2	Historical background
3	Quantities of information
3.1	Entropy of an information source
3.2	Joint entropy
3.3	Conditional entropy (equivocation)
3.4	Mutual information (transinformation)
3.5	Kullback–Leibler divergence (information gain)
3.6	Other quantities
4	Coding theory
4.1	Source theory
4.1.1	Rate
4.2	Channel capacity
4.2.1	Capacity of particular channel models
5	Applications to other fields
5.1	Intelligence uses and secrecy applications
5.2	Pseudorandom number generation
5.3	Seismic exploration
5.4	Semiotics
5.5	Miscellaneous applications
6	See also
6.1	Applications
6.2	History
6.3	Theory
6.4	Concepts
7	References
7.1	The classic work
7.2	Other journal articles
7.3	Textbooks on information theory
7.4	Other books
7.5	MOOC on information theory
8	External links
Overview
Information theory studies the transmission, processing, extraction, and utilization of information. Abstractly, information can be thought of as the resolution of uncertainty. In the case of communication of information over a noisy channel, this abstract concept was made concrete in 1948 by Claude Shannon in his paper "A Mathematical Theory of Communication", in which "information" is thought of as a set of possible messages, where the goal is to send these messages over a noisy channel, and then to have the receiver reconstruct the message with low probability of error, in spite of the channel noise. Shannon's main result, the noisy-channel coding theorem showed that, in the limit of many channel uses, the rate of information that is asymptotically achievable is equal to the channel capacity, a quantity dependent merely on the statistics of the channel over which the messages are sent.[1]

Information theory is closely associated with a collection of pure and applied disciplines that have been investigated and reduced to engineering practice under a variety of rubrics throughout the world over the past half century or more: adaptive systems, anticipatory systems, artificial intelligence, complex systems, complexity science, cybernetics, informatics, machine learning, along with systems sciences of many descriptions. Information theory is a broad and deep mathematical theory, with equally broad and deep applications, amongst which is the vital field of coding theory.

Coding theory is concerned with finding explicit methods, called codes, for increasing the efficiency and reducing the error rate of data communication over noisy channels to near the channel capacity. These codes can be roughly subdivided into data compression (source coding) and error-correction (channel coding) techniques. In the latter case, it took many years to find the methods Shannon's work proved were possible. A third class of information theory codes are cryptographic algorithms (both codes and ciphers). Concepts, methods and results from coding theory and information theory are widely used in cryptography and cryptanalysis. See the article ban (unit) for a historical application.

Information theory is also used in information retrieval, intelligence gathering, gambling, statistics, and even in musical composition.

Historical background
Main article: History of information theory
The landmark event that established the discipline of information theory and brought it to immediate worldwide attention was the publication of Claude E. Shannon's classic paper "A Mathematical Theory of Communication" in the Bell System Technical Journal in July and October 1948.

Prior to this paper, limited information-theoretic ideas had been developed at Bell Labs, all implicitly assuming events of equal probability. Harry Nyquist's 1924 paper, Certain Factors Affecting Telegraph Speed, contains a theoretical section quantifying "intelligence" and the "line speed" at which it can be transmitted by a communication system, giving the relation W = K log m (recalling Boltzmann's constant), where W is the speed of transmission of intelligence, m is the number of different voltage levels to choose from at each time step, and K is a constant. Ralph Hartley's 1928 paper, Transmission of Information, uses the word information as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as H = log Sn = n log S, where S was the number of possible symbols, and n the number of symbols in a transmission. The unit of information was therefore the decimal digit, which has since sometimes been called the hartley in his honor as a unit or scale or measure of information. Alan Turing in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers.

Much of the mathematics behind information theory with events of different probabilities were developed for the field of thermodynamics by Ludwig Boltzmann and J. Willard Gibbs. Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by Rolf Landauer in the 1960s, are explored in Entropy in thermodynamics and information theory.

In Shannon's revolutionary and groundbreaking paper, the work for which had been substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion that

"The fundamental problem of communication is that of reproducing at one point, either exactly or approximately, a message selected at another point."
With it came the ideas of

the information entropy and redundancy of a source, and its relevance through the source coding theorem;
the mutual information, and the channel capacity of a noisy channel, including the promise of perfect loss-free communication given by the noisy-channel coding theorem;
the practical result of the Shannon–Hartley law for the channel capacity of a Gaussian channel; as well as
the bit—a new way of seeing the most fundamental unit of information.
Quantities of information
Main article: Quantities of information
Information theory is based on probability theory and statistics. Information theory often concerns itself with measures of information of the distributions associated with random variables. Important quantities of information are entropy, a measure of information in a single random variable, and mutual information, a measure of information in common between two random variables. The former quantity is a property of the probability distribution of a random variable and gives a limit on the rate at which data generated by independent samples with the given distribution can be reliably compressed. The latter is a property of the joint distribution of two random variables, and is the maximum rate of reliable communication across a noisy channel in the limit of long block lengths, when the channel statistics are determined by the joint distribution.

The choice of logarithmic base in the following formulae determines the unit of information entropy that is used. A common unit of information is the bit, based on the binary logarithm. Other units include the nat, which is based on the natural logarithm, and the decimal digit, which is based on the common logarithm.

In what follows, an expression of the form p log p is considered by convention to be equal to zero whenever p = 0. This is justified because {\displaystyle \lim _{p\rightarrow 0+}p\log p=0} \lim _{p\rightarrow 0+}p\log p=0 for any logarithmic base.

Entropy of an information source
Based on the probability mass function of each source symbol to be communicated, the Shannon entropy H, in units of bits (per symbol), is given by

{\displaystyle H=-\sum _{i}p_{i}\log _{2}(p_{i})} {\displaystyle H=-\sum _{i}p_{i}\log _{2}(p_{i})}
where pi is the probability of occurrence of the i-th possible value of the source symbol. This equation gives the entropy in the units of "bits" (per symbol) because it uses a logarithm of base 2, and this base-2 measure of entropy has sometimes been called the "shannon" in his honor. Entropy is also commonly computed using the natural logarithm (base e, where e is Euler's number), which produces a measurement of entropy in "nats" per symbol and sometimes simplifies the analysis by avoiding the need to include extra constants in the formulas. Other bases are also possible, but less commonly used. For example, a logarithm of base 28 = 256 will produce a measurement in bytes per symbol, and a logarithm of base 10 will produce a measurement in decimal digits (or hartleys) per symbol.

Intuitively, the entropy HX of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known.

The entropy of a source that emits a sequence of N symbols that are independent and identically distributed (iid) is N·H bits (per message of N symbols). If the source data symbols are identically distributed but not independent, the entropy of a message of length N will be less than N·H.


The entropy of a Bernoulli trial as a function of success probability, often called the binary entropy function, Hb(p). The entropy is maximized at 1 bit per trial when the two possible outcomes are equally probable, as in an unbiased coin toss.
If one transmits 1000 bits (0s and 1s), and the value of each of these bits is known to the receiver (has a specific value with certainty) ahead of transmission, it is clear that no information is transmitted. If, however, each bit is independently equally likely to be 0 or 1, 1000 shannons of information (more often called bits) have been transmitted. Between these two extremes, information can be quantified as follows. If ?? is the set of all messages {x1, …, xn} that X could be, and p(x) is the probability of some {\displaystyle x\in \mathbb {X} } x\in \mathbb {X} , then the entropy, H, of X is defined:[9]

{\displaystyle H(X)=\mathbb {E} _{X}[I(x)]=-\sum _{x\in \mathbb {X} }p(x)\log p(x).} H(X)=\mathbb {E} _{X}[I(x)]=-\sum _{x\in \mathbb {X} }p(x)\log p(x).
(Here, I(x) is the self-information, which is the entropy contribution of an individual message, and ??X is the expected value.) A property of entropy is that it is maximized when all the messages in the message space are equiprobable p(x) = 1/n; i.e., most unpredictable, in which case H(X) = log n.

The special case of information entropy for a random variable with two outcomes is the binary entropy function, usually taken to the logarithmic base 2, thus having the shannon (Sh) as unit:

{\displaystyle H_{\mathrm {b} }(p)=-p\log _{2}p-(1-p)\log _{2}(1-p).} H_{\mathrm {b} }(p)=-p\log _{2}p-(1-p)\log _{2}(1-p).
Joint entropy
The joint entropy of two discrete random variables X and Y is merely the entropy of their pairing: (X, Y). This implies that if X and Y are independent, then their joint entropy is the sum of their individual entropies.

For example, if (X, Y) represents the position of a chess piece — X the row and Y the column, then the joint entropy of the row of the piece and the column of the piece will be the entropy of the position of the piece.

{\displaystyle H(X,Y)=\mathbb {E} _{X,Y}[-\log p(x,y)]=-\sum _{x,y}p(x,y)\log p(x,y)\,} H(X,Y)=\mathbb {E} _{X,Y}[-\log p(x,y)]=-\sum _{x,y}p(x,y)\log p(x,y)\,
Despite similar notation, joint entropy should not be confused with cross entropy.

Conditional entropy (equivocation)
The conditional entropy or conditional uncertainty of X given random variable Y (also called the equivocation of X about Y) is the average conditional entropy over Y:[10]

{\displaystyle H(X|Y)=\mathbb {E} _{Y}[H(X|y)]=-\sum _{y\in Y}p(y)\sum _{x\in X}p(x|y)\log p(x|y)=-\sum _{x,y}p(x,y)\log p(x|y).} {\displaystyle H(X|Y)=\mathbb {E} _{Y}[H(X|y)]=-\sum _{y\in Y}p(y)\sum _{x\in X}p(x|y)\log p(x|y)=-\sum _{x,y}p(x,y)\log p(x|y).}
Because entropy can be conditioned on a random variable or on that random variable being a certain value, care should be taken not to confuse these two definitions of conditional entropy, the former of which is in more common use. A basic property of this form of conditional entropy is that:

{\displaystyle H(X|Y)=H(X,Y)-H(Y).\,} H(X|Y)=H(X,Y)-H(Y).\,
Mutual information (transinformation)
Mutual information measures the amount of information that can be obtained about one random variable by observing another. It is important in communication where it can be used to maximize the amount of information shared between sent and received signals. The mutual information of X relative to Y is given by:

{\displaystyle I(X;Y)=\mathbb {E} _{X,Y}[SI(x,y)]=\sum _{x,y}p(x,y)\log {\frac {p(x,y)}{p(x)\,p(y)}}} I(X;Y)=\mathbb {E} _{X,Y}[SI(x,y)]=\sum _{x,y}p(x,y)\log {\frac {p(x,y)}{p(x)\,p(y)}}
where SI (Specific mutual Information) is the pointwise mutual information.

A basic property of the mutual information is that

{\displaystyle I(X;Y)=H(X)-H(X|Y).\,} I(X;Y)=H(X)-H(X|Y).\,
That is, knowing Y, we can save an average of I(X; Y) bits in encoding X compared to not knowing Y.

Mutual information is symmetric:

{\displaystyle I(X;Y)=I(Y;X)=H(X)+H(Y)-H(X,Y).\,} I(X;Y)=I(Y;X)=H(X)+H(Y)-H(X,Y).\,
Mutual information can be expressed as the average Kullback–Leibler divergence (information gain) between the posterior probability distribution of X given the value of Y and the prior distribution on X:

{\displaystyle I(X;Y)=\mathbb {E} _{p(y)}[D_{\mathrm {KL} }(p(X|Y=y)\|p(X))].} I(X;Y)=\mathbb {E} _{p(y)}[D_{\mathrm {KL} }(p(X|Y=y)\|p(X))].
In other words, this is a measure of how much, on the average, the probability distribution on X will change if we are given the value of Y. This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution:

{\displaystyle I(X;Y)=D_{\mathrm {KL} }(p(X,Y)\|p(X)p(Y)).} I(X;Y)=D_{\mathrm {KL} }(p(X,Y)\|p(X)p(Y)).
Mutual information is closely related to the log-likelihood ratio test in the context of contingency tables and the multinomial distribution and to Pearson's ?2 test: mutual information can be considered a statistic for assessing independence between a pair of variables, and has a well-specified asymptotic distribution.

Kullback–Leibler divergence (information gain)
The Kullback–Leibler divergence (or information divergence, information gain, or relative entropy) is a way of comparing two distributions: a "true" probability distribution p(X), and an arbitrary probability distribution q(X). If we compress data in a manner that assumes q(X) is the distribution underlying some data, when, in reality, p(X) is the correct distribution, the Kullback–Leibler divergence is the number of average additional bits per datum necessary for compression. It is thus defined

{\displaystyle D_{\mathrm {KL} }(p(X)\|q(X))=\sum _{x\in X}-p(x)\log {q(x)}\,-\,\sum _{x\in X}-p(x)\log {p(x)}=\sum _{x\in X}p(x)\log {\frac {p(x)}{q(x)}}.} D_{\mathrm {KL} }(p(X)\|q(X))=\sum _{x\in X}-p(x)\log {q(x)}\,-\,\sum _{x\in X}-p(x)\log {p(x)}=\sum _{x\in X}p(x)\log {\frac {p(x)}{q(x)}}.
Although it is sometimes used as a 'distance metric', KL divergence is not a true metric since it is not symmetric and does not satisfy the triangle inequality (making it a semi-quasimetric).

Another interpretation of the KL divergence is the "unnecessary surprise" introduced by a prior from the truth: suppose a number X is about to be drawn randomly from a discrete set with probability distribution p(x). If Alice knows the true distribution p(x), while Bob believes (has a prior) that the distribution is q(x), then Bob will be more surprised than Alice, on average, upon seeing the value of X. The KL divergence is the (objective) expected value of Bob's (subjective) surprisal minus Alice's surprisal, measured in bits if the log is in base 2. In this way, the extent to which Bob's prior is "wrong" can be quantified in terms of how "unnecessarily surprised" it is expected to make him.

Other quantities
Other important information theoretic quantities include Rényi entropy (a generalization of entropy), differential entropy (a generalization of quantities of information to continuous distributions), and the conditional mutual information.

Coding theory
Main article: Coding theory

A picture showing scratches on the readable surface of a CD-R. Music and data CDs are coded using error correcting codes and thus can still be read even if they have minor scratches using error detection and correction.
Coding theory is one of the most important and direct applications of information theory. It can be subdivided into source coding theory and channel coding theory. Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source.

Data compression (source coding): There are two formulations for the compression problem:
lossless data compression: the data must be reconstructed exactly;
lossy data compression: allocates bits needed to reconstruct the data, within a specified fidelity level measured by a distortion function. This subset of information theory is called rate–distortion theory.
Error-correcting codes (channel coding): While data compression removes as much redundancy as possible, an error correcting code adds just the right kind of redundancy (i.e., error correction) needed to transmit the data efficiently and faithfully across a noisy channel.
This division of coding theory into compression and transmission is justified by the information transmission theorems, or source–channel separation theorems that justify the use of bits as the universal currency for information in many contexts. However, these theorems only hold in the situation where one transmitting user wishes to communicate to one receiving user. In scenarios with more than one transmitter (the multiple-access channel), more than one receiver (the broadcast channel) or intermediary "helpers" (the relay channel), or more general networks, compression followed by transmission may no longer be optimal. Network information theory refers to these multi-agent communication models.

Source theory
Any process that generates successive messages can be considered a source of information. A memoryless source is one in which each message is an independent identically distributed random variable, whereas the properties of ergodicity and stationarity impose less restrictive constraints. All such sources are stochastic. These terms are well studied in their own right outside information theory.

Rate
Information rate is the average entropy per symbol. For memoryless sources, this is merely the entropy of each symbol, while, in the case of a stationary stochastic process, it is

{\displaystyle r=\lim _{n\to \infty }H(X_{n}|X_{n-1},X_{n-2},X_{n-3},\ldots );} r=\lim _{n\to \infty }H(X_{n}|X_{n-1},X_{n-2},X_{n-3},\ldots );
that is, the conditional entropy of a symbol given all the previous symbols generated. For the more general case of a process that is not necessarily stationary, the average rate is

{\displaystyle r=\lim _{n\to \infty }{\frac {1}{n}}H(X_{1},X_{2},\dots X_{n});} r=\lim _{n\to \infty }{\frac {1}{n}}H(X_{1},X_{2},\dots X_{n});
that is, the limit of the joint entropy per symbol. For stationary sources, these two expressions give the same result.[11]

It is common in information theory to speak of the "rate" or "entropy" of a language. This is appropriate, for example, when the source of information is English prose. The rate of a source of information is related to its redundancy and how well it can be compressed, the subject of source coding.

Channel capacity
Main article: Channel capacity
Communications over a channel—such as an ethernet cable—is the primary motivation of information theory. As anyone who's ever used a telephone (mobile or landline) knows, however, such channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.

Consider the communications process over a discrete channel. A simple model of the process is shown below:

Comm Channel.svg
Here X represents the space of messages transmitted, and Y the space of messages received during a unit time over our channel. Let p(y|x) be the conditional probability distribution function of Y given X. We will consider p(y|x) to be an inherent fixed property of our communications channel (representing the nature of the noise of our channel). Then the joint distribution of X and Y is completely determined by our channel and by our choice of f(x), the marginal distribution of messages we choose to send over the channel. Under these constraints, we would like to maximize the rate of information, or the signal, we can communicate over the channel. The appropriate measure for this is the mutual information, and this maximum mutual information is called the channel capacity and is given by:

{\displaystyle C=\max _{f}I(X;Y).\!} C=\max _{f}I(X;Y).\!
This capacity has the following property related to communicating at information rate R (where R is usually bits per symbol). For any information rate R < C and coding error e > 0, for large enough N, there exists a code of length N and rate = R and a decoding algorithm, such that the maximal probability of block error is = e; that is, it is always possible to transmit with arbitrarily small block error. In addition, for any rate R > C, it is impossible to transmit with arbitrarily small block error.

Channel coding is concerned with finding such nearly optimal codes that can be used to transmit data over a noisy channel with a small coding error at a rate near the channel capacity.

Capacity of particular channel models
A continuous-time analog communications channel subject to Gaussian noise — see Shannon–Hartley theorem.
A binary symmetric channel (BSC) with crossover probability p is a binary input, binary output channel that flips the input bit with probability p. The BSC has a capacity of 1 - Hb(p) bits per channel use, where Hb is the binary entropy function to the base-2 logarithm:
Binary symmetric channel.svg
A binary erasure channel (BEC) with erasure probability p is a binary input, ternary output channel. The possible channel outputs are 0, 1, and a third symbol 'e' called an erasure. The erasure represents complete loss of information about an input bit. The capacity of the BEC is 1 - p bits per channel use.
Binary erasure channel.svg
Applications to other fields
Intelligence uses and secrecy applications
Information theoretic concepts apply to cryptography and cryptanalysis. Turing's information unit, the ban, was used in the Ultra project, breaking the German Enigma machine code and hastening the end of World War II in Europe. Shannon himself defined an important concept now called the unicity distance. Based on the redundancy of the plaintext, it attempts to give a minimum amount of ciphertext necessary to ensure unique decipherability.

Information theory leads us to believe it is much more difficult to keep secrets than it might first appear. A brute force attack can break systems based on asymmetric key algorithms or on most commonly used methods of symmetric key algorithms (sometimes called secret key algorithms), such as block ciphers. The security of all such methods currently comes from the assumption that no known attack can break them in a practical amount of time.

Information theoretic security refers to methods such as the one-time pad that are not vulnerable to such brute force attacks. In such cases, the positive conditional mutual information between the plaintext and ciphertext (conditioned on the key) can ensure proper transmission, while the unconditional mutual information between the plaintext and ciphertext remains zero, resulting in absolutely secure communications. In other words, an eavesdropper would not be able to improve his or her guess of the plaintext by gaining knowledge of the ciphertext but not of the key. However, as in any other cryptographic system, care must be used to correctly apply even information-theoretically secure methods; the Venona project was able to crack the one-time pads of the Soviet Union due to their improper reuse of key material.

Pseudorandom number generation
Pseudorandom number generators are widely available in computer language libraries and application programs. They are, almost universally, unsuited to cryptographic use as they do not evade the deterministic nature of modern computer equipment and software. A class of improved random number generators is termed cryptographically secure pseudorandom number generators, but even they require random seeds external to the software to work as intended. These can be obtained via extractors, if done carefully. The measure of sufficient randomness in extractors is min-entropy, a value related to Shannon entropy through Rényi entropy; Rényi entropy is also used in evaluating randomness in cryptographic systems. Although related, the distinctions among these measures mean that a random variable with high Shannon entropy is not necessarily satisfactory for use in an extractor and so for cryptography uses.

Seismic exploration
One early commercial application of information theory was in the field of seismic oil exploration. Work in this field made it possible to strip off and separate the unwanted noise from the desired seismic signal. Information theory and digital signal processing offer a major improvement of resolution and image clarity over previous analog methods.[12]

Semiotics
Concepts from information theory such as redundancy and code control have been used by semioticians such as Umberto Eco and Ferruccio Rossi-Landi to explain ideology as a form of message transmission whereby a dominant social class emits its message by using signs that exhibit a high degree of redundancy such that only one message is decoded among a selection of competing ones.[13]

Miscellaneous applications
Information theory also has applications in gambling and investing, black holes, and bioinformatics.

See also
icon	Mathematics portal
Algorithmic probability
Algorithmic information theory
Bayesian inference
Communication theory
Constructor theory - a generalization of information theory that includes quantum information
Inductive probability
Info-metrics
Minimum message length
Minimum description length
List of important publications
Philosophy of information
Applications
Active networking
Cryptanalysis
Cryptography
Cybernetics
Entropy in thermodynamics and information theory
Gambling
Intelligence (information gathering)
Seismic exploration
History
Hartley, R.V.L.
History of information theory
Shannon, C.E.
Timeline of information theory
Yockey, H.P.
Theory
Coding theory
Detection theory
Estimation theory
Fisher information
Information algebra
Information asymmetry
Information field theory
Information geometry
Information theory and measure theory
Kolmogorov complexity
List of unsolved problems in information theory
Logic of information
Network coding
Philosophy of information
Quantum information science
Semiotic information theory
Source coding
Concepts
Ban (unit)
Channel capacity
Communication channel
Communication source
Conditional entropy
Covert channel
Decoder
Differential entropy
Encoder
Information entropy
Joint entropy
Kullback–Leibler divergence
Mutual information
Pointwise mutual information (PMI)
Receiver (information theory)
Redundancy
Rényi entropy
Self-information
Unicity distance
Variety
Hamming distance
References
 F. Rieke; D. Warland; R Ruyter van Steveninck; W Bialek (1997). Spikes: Exploring the Neural Code. The MIT press. ISBN 978-0262681087.
 Delgado-Bonal, Alfonso; Martín-Torres, Javier (2016-11-03). "Human vision is determined based on information theory". Scientific Reports. 6 (1). Bibcode:2016NatSR...636038D. doi:10.1038/srep36038. ISSN 2045-2322.
 cf; Huelsenbeck, J. P.; Ronquist, F.; Nielsen, R.; Bollback, J. P. (2001). "Bayesian inference of phylogeny and its impact on evolutionary biology". Science. 294 (5550): 2310–2314. Bibcode:2001Sci...294.2310H. doi:10.1126/science.1065889.
 Allikmets, Rando; Wasserman, Wyeth W.; Hutchinson, Amy; Smallwood, Philip; Nathans, Jeremy; Rogan, Peter K. (1998). "Thomas D. Schneider], Michael Dean (1998) Organization of the ABCR gene: analysis of promoter and splice junction sequences". Gene. 215 (1): 111–122. doi:10.1016/s0378-1119(98)00269-8.
 Burnham, K. P. and Anderson D. R. (2002) Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach, Second Edition (Springer Science, New York) ISBN 978-0-387-95364-9.
 Jaynes, E. T. (1957). "Information Theory and Statistical Mechanics". Phys. Rev. 106 (4): 620. Bibcode:1957PhRv..106..620J. doi:10.1103/physrev.106.620.
 Bennett, Charles H.; Li, Ming; Ma, Bin (2003). "Chain Letters and Evolutionary Histories". Scientific American. 288 (6): 76–81. Bibcode:2003SciAm.288f..76B. doi:10.1038/scientificamerican0603-76. PMID 12764940.
 David R. Anderson (November 1, 2003). "Some background on why people in the empirical sciences may want to better understand the information-theoretic methods" (PDF). Archived from the original (pdf) on July 23, 2011. Retrieved 2010-06-23.
 Fazlollah M. Reza (1994) [1961]. An Introduction to Information Theory. Dover Publications, Inc., New York. ISBN 0-486-68210-2.
 Robert B. Ash (1990) [1965]. Information Theory. Dover Publications, Inc. ISBN 0-486-66521-6.
 Jerry D. Gibson (1998). Digital Compression for Multimedia: Principles and Standards. Morgan Kaufmann. ISBN 1-55860-369-7.
 The Corporation and Innovation, Haggerty, Patrick, Strategic Management Journal, Vol. 2, 97-118 (1981)
 Semiotics of Ideology, Noth, Winfried, Semiotica, Issue 148,(1981)
The classic work
Shannon, C.E. (1948), "A Mathematical Theory of Communication", Bell System Technical Journal, 27, pp. 379–423 & 623–656, July & October, 1948. PDF. 
Notes and other formats.
R.V.L. Hartley, "Transmission of Information", Bell System Technical Journal, July 1928
Andrey Kolmogorov (1968), "Three approaches to the quantitative definition of information" in International Journal of Computer Mathematics.
Other journal articles
J. L. Kelly, Jr., Betbubbles.com, "A New Interpretation of Information Rate" Bell System Technical Journal, Vol. 35, July 1956, pp. 917–26.
R. Landauer, IEEE.org, "Information is Physical" Proc. Workshop on Physics and Computation PhysComp'92 (IEEE Comp. Sci.Press, Los Alamitos, 1993) pp. 1–4.
R. Landauer, IBM.com, "Irreversibility and Heat Generation in the Computing Process" IBM J. Res. Develop. Vol. 5, No. 3, 1961
Timme, Nicholas; Alford, Wesley; Flecker, Benjamin; Beggs, John M. (2012). "Multivariate information measures: an experimentalist's perspective". arXiv:1111.6857?Freely accessible [cs.IT].
Textbooks on information theory
Arndt, C. Information Measures, Information and its Description in Science and Engineering (Springer Series: Signals and Communication Technology), 2004, ISBN 978-3-540-40855-0
Ash, RB. Information Theory. New York: Interscience, 1965. ISBN 0-470-03445-9. New York: Dover 1990. ISBN 0-486-66521-6
Gallager, R. Information Theory and Reliable Communication. New York: John Wiley and Sons, 1968. ISBN 0-471-29048-3
Goldman, S. Information Theory. New York: Prentice Hall, 1953. New York: Dover 1968 ISBN 0-486-62209-6, 2005 ISBN 0-486-44271-3
Cover, Thomas; Thomas, Joy A. (2006). Elements of information theory (2nd ed.). New York: Wiley-Interscience. ISBN 0-471-24195-4.
Csiszar, I, Korner, J. Information Theory: Coding Theorems for Discrete Memoryless Systems Akademiai Kiado: 2nd edition, 1997. ISBN 963-05-7440-3
MacKay, David J. C.. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1
Mansuripur, M. Introduction to Information Theory. New York: Prentice Hall, 1987. ISBN 0-13-484668-0
McEliece, R. The Theory of Information and Coding". Cambridge, 2002. ISBN 978-0521831857
Pierce, JR. "An introduction to information theory: symbols, signals and noise". Dover (2nd Edition). 1961 (reprinted by Dover 1980).
Reza, F. An Introduction to Information Theory. New York: McGraw-Hill 1961. New York: Dover 1994. ISBN 0-486-68210-2
Shannon, Claude; Weaver, Warren (1949). The Mathematical Theory of Communication (PDF). Urbana, Illinois: University of Illinois Press. ISBN 0-252-72548-4. LCCN 49-11922.
Stone, JV. Chapter 1 of book "Information Theory: A Tutorial Introduction", University of Sheffield, England, 2014. ISBN 978-0956372857.
Yeung, RW. A First Course in Information Theory Kluwer Academic/Plenum Publishers, 2002. ISBN 0-306-46791-7.
Yeung, RW. Information Theory and Network Coding Springer 2008, 2002. ISBN 978-0-387-79233-0
Other books
Leon Brillouin, Science and Information Theory, Mineola, N.Y.: Dover, [1956, 1962] 2004. ISBN 0-486-43918-6
James Gleick, The Information: A History, a Theory, a Flood, New York: Pantheon, 2011. ISBN 978-0-375-42372-7
A. I. Khinchin, Mathematical Foundations of Information Theory, New York: Dover, 1957. ISBN 0-486-60434-9
H. S. Leff and A. F. Rex, Editors, Maxwell's Demon: Entropy, Information, Computing, Princeton University Press, Princeton, New Jersey (1990). ISBN 0-691-08727-X
Robert K. Logan. What is Information? - Propagating Organization in the Biosphere, the Symbolosphere, the Technosphere and the Econosphere,
Toronto: DEMO Publishing.

Tom Siegfried, The Bit and the Pendulum, Wiley, 2000. ISBN 0-471-32174-5
Charles Seife, Decoding the Universe, Viking, 2006. ISBN 0-670-03441-X
Jeremy Campbell, Grammatical Man, Touchstone/Simon & Schuster, 1982, ISBN 0-671-44062-4
Henri Theil, Economics and Information Theory, Rand McNally & Company - Chicago, 1967.
Escolano, Suau, Bonev, Information Theory in Computer Vision and Pattern Recognition, Springer, 2009. ISBN 978-1-84882-296-2
Vlatko Vedral, Decoding Reality: The Universe as Quantum Information, Oxford University Press 2010. ISBN 0-19-923769-7
MOOC on information theory
	Wikiquote has quotations related to: Information theory
Library resources about 
Information theory
Resources in your library
Resources in other libraries
Raymond W. Yeung, "Information Theory" (The Chinese University of Hong Kong)
External links
	Wikiquote has quotations related to: Information theory
Library resources about 
Information theory
Resources in your library
Resources in other libraries
Hazewinkel, Michiel, ed. (2001) [1994], "Information", Encyclopedia of Mathematics, Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN 978-1-55608-010-4
Lambert F. L. (1999), "Shuffled Cards, Messy Desks, and Disorderly Dorm Rooms - Examples of Entropy Increase? Nonsense!", Journal of Chemical Education
IEEE Information Theory Society and ITSOC Monographs, Surveys, and Reviews
vte
Subfields of and scientists involved in cybernetics
vte
Data compression methods
vte
Areas of mathematics
vte
Major fields of computer science
Authority control Edit this at Wikidata	
GND: 4026927-9 NDL: 00575012
Categories: Information theoryCyberneticsFormal sciencesInformation Age
Navigation menu
Not logged inTalkContributionsCreate accountLog inArticleTalkReadEditView historySearch

Search Wikipedia
Main page
Contents
Featured content
Current events
Random article
Donate to Wikipedia
Wikipedia store
Interaction
Help
About Wikipedia
Community portal
Recent changes
Contact page
Tools
What links here
Related changes
Upload file
Special pages
Permanent link
Page information
Wikidata item
Cite this page
Print/export
Create a book
Download as PDF
Printable version
In other projects
Wikimedia Commons
Wikiquote

Languages
Deutsch
Español
Français
???
Italiano
???????
Tagalog
Ti?ng Vi?t
??
54 more
Edit links
This page was last edited on 25 June 2018, at 17:17 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaDevelopersCookie statementMobile viewWikimedia Foundation Powered by MediaWiki


 This is a good article. Follow the link for more information.
Hidden Markov model
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Machine learning and
data mining
Kernel Machine.svg
Problems[show]
Supervised learning
(classification • regression)
[show]
Clustering[show]
Dimensionality reduction[show]
Structured prediction[hide]
Graphical models (Bayes net, CRF, HMM)
Anomaly detection[show]
Neural nets[show]
Reinforcement learning[show]
Theory[show]
Machine-learning venues[show]
Glossary of artificial intelligence[show]
Related articles[show]
Portal-puzzle.svg Machine learning portal
vte
Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states.

The hidden Markov model can be represented as the simplest dynamic Bayesian network. The mathematics behind the HMM were developed by L. E. Baum and coworkers. [1][2][3][4][5] HMM is closely related to earlier work on the optimal nonlinear filtering problem by Ruslan L. Stratonovich,[6] who was the first to describe the forward-backward procedure.

In simpler Markov models (like a Markov chain), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters, while in the hidden Markov model, the state is not directly visible, but the output (in the form of data or "token" in the following), dependent on the state, is visible. Each state has a probability distribution over the possible output tokens. Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states; this is also known as pattern theory, a topic of grammar induction.

The adjective hidden refers to the state sequence through which the model passes, not to the parameters of the model; the model is still referred to as a hidden Markov model even if these parameters are known exactly.

Hidden Markov models are especially known for their application in reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition,[7] part-of-speech tagging, musical score following,[8] partial discharges[9] and bioinformatics[10].

A hidden Markov model can be considered a generalization of a mixture model where the hidden variables (or latent variables), which control the mixture component to be selected for each observation, are related through a Markov process rather than independent of each other. Recently, hidden Markov models have been generalized to pairwise Markov models and triplet Markov models which allow consideration of more complex data structures[11][12] and the modeling of nonstationary data.[13][14]


Contents
1	Description in terms of urns
2	Architecture
3	Inference
3.1	Probability of an observed sequence
3.2	Probability of the latent variables
3.2.1	Filtering
3.2.2	Smoothing
3.2.3	Most likely explanation
3.3	Statistical significance
4	A concrete example
5	Learning
6	Mathematical description
6.1	General description
6.2	Compared with a simple mixture model
6.3	Examples
6.4	A two-level Bayesian HMM
6.5	Poisson hidden Markov model
7	Applications
8	History
9	Types
10	Extensions
11	See also
12	References
13	External links
13.1	Concepts
13.2	Software
Description in terms of urns

Figure 1. Probabilistic parameters of a hidden Markov model (example)
X — states
y — possible observations
a — state transition probabilities
b — output probabilities
In its discrete form, a hidden Markov process can be visualized as a generalization of the Urn problem with replacement (where each item from the urn is returned to the original urn before the next step).[15] Consider this example: in a room that is not visible to an observer there is a genie. The room contains urns X1, X2, X3, … each of which contains a known mix of balls, each ball labeled y1, y2, y3, … . The genie chooses an urn in that room and randomly draws a ball from that urn. It then puts the ball onto a conveyor belt, where the observer can observe the sequence of the balls but not the sequence of urns from which they were drawn. The genie has some procedure to choose urns; the choice of the urn for the n-th ball depends only upon a random number and the choice of the urn for the (n - 1)-th ball. The choice of urn does not directly depend on the urns chosen before this single previous urn; therefore, this is called a Markov process. It can be described by the upper part of Figure 1.

The Markov process itself cannot be observed, only the sequence of labeled balls, thus this arrangement is called a "hidden Markov process". This is illustrated by the lower part of the diagram shown in Figure 1, where one can see that balls y1, y2, y3, y4 can be drawn at each state. Even if the observer knows the composition of the urns and has just observed a sequence of three balls, e.g. y1, y2 and y3 on the conveyor belt, the observer still cannot be sure which urn (i.e., at which state) the genie has drawn the third ball from. However, the observer can work out other information, such as the likelihood that the third ball came from each of the urns.

Architecture
The diagram below shows the general architecture of an instantiated HMM. Each oval shape represents a random variable that can adopt any of a number of values. The random variable x(t) is the hidden state at time t (with the model from the above diagram, x(t) ? { x1, x2, x3 }). The random variable y(t) is the observation at time t (with y(t) ? { y1, y2, y3, y4 }). The arrows in the diagram (often called a trellis diagram) denote conditional dependencies.

From the diagram, it is clear that the conditional probability distribution of the hidden variable x(t) at time t, given the values of the hidden variable x at all times, depends only on the value of the hidden variable x(t - 1); the values at time t - 2 and before have no influence. This is called the Markov property. Similarly, the value of the observed variable y(t) only depends on the value of the hidden variable x(t) (both at time t).

In the standard type of hidden Markov model considered here, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). The parameters of a hidden Markov model are of two types, transition probabilities and emission probabilities (also known as output probabilities). The transition probabilities control the way the hidden state at time t is chosen given the hidden state at time {\displaystyle t-1} t-1.

The hidden state space is assumed to consist of one of N possible values, modeled as a categorical distribution. (See the section below on extensions for other possibilities.) This means that for each of the N possible states that a hidden variable at time t can be in, there is a transition probability from this state to each of the N possible states of the hidden variable at time {\displaystyle t+1} t+1, for a total of {\displaystyle N^{2}} N^{2} transition probabilities. Note that the set of transition probabilities for transitions from any given state must sum to 1. Thus, the {\displaystyle N\times N} N\times N matrix of transition probabilities is a Markov matrix. Because any one transition probability can be determined once the others are known, there are a total of {\displaystyle N(N-1)} N(N-1) transition parameters.

In addition, for each of the N possible states, there is a set of emission probabilities governing the distribution of the observed variable at a particular time given the state of the hidden variable at that time. The size of this set depends on the nature of the observed variable. For example, if the observed variable is discrete with M possible values, governed by a categorical distribution, there will be {\displaystyle M-1} M-1 separate parameters, for a total of {\displaystyle N(M-1)} N(M-1) emission parameters over all hidden states. On the other hand, if the observed variable is an M-dimensional vector distributed according to an arbitrary multivariate Gaussian distribution, there will be M parameters controlling the means and {\displaystyle {\frac {M(M+1)}{2}}} {\frac {M(M+1)}{2}} parameters controlling the covariance matrix, for a total of {\displaystyle N\left(M+{\frac {M(M+1)}{2}}\right)={\frac {NM(M+3)}{2}}=O(NM^{2})} N\left(M+{\frac {M(M+1)}{2}}\right)={\frac {NM(M+3)}{2}}=O(NM^{2}) emission parameters. (In such a case, unless the value of M is small, it may be more practical to restrict the nature of the covariances between individual elements of the observation vector, e.g. by assuming that the elements are independent of each other, or less restrictively, are independent of all but a fixed number of adjacent elements.)

Temporal evolution of a hidden Markov model
Inference

The state transition and output probabilities of an HMM are indicated by the line opacity in the upper part of the diagram. Given that we have observed the output sequence in the lower part of the diagram, we may be interested in the most likely sequence of states that could have produced it. Based on the arrows that are present in the diagram, the following state sequences are candidates:
5 3 2 5 3 2
4 3 2 5 3 2
3 1 2 5 3 2
We can find the most likely sequence by evaluating the joint probability of both the state sequence and the observations for each case (simply by multiplying the probability values, which here correspond to the opacities of the arrows involved). In general, this type of problem (i.e. finding the most likely explanation for an observation sequence) can be solved efficiently using the Viterbi algorithm.
Several inference problems are associated with hidden Markov models, as outlined below.

Probability of an observed sequence
The task is to compute in a best way, given the parameters of the model, the probability of a particular output sequence. This requires summation over all possible state sequences:

The probability of observing a sequence

{\displaystyle Y=y(0),y(1),\dots ,y(L-1)\,} Y=y(0),y(1),\dots ,y(L-1)\,
of length L is given by

{\displaystyle P(Y)=\sum _{X}P(Y\mid X)P(X),\,} P(Y)=\sum _{X}P(Y\mid X)P(X),\,
where the sum runs over all possible hidden-node sequences

{\displaystyle X=x(0),x(1),\dots ,x(L-1).\,} X=x(0),x(1),\dots ,x(L-1).\,
Applying the principle of dynamic programming, this problem, too, can be handled efficiently using the forward algorithm.

Probability of the latent variables
A number of related tasks ask about the probability of one or more of the latent variables, given the model's parameters and a sequence of observations {\displaystyle y(1),\dots ,y(t).} y(1),\dots ,y(t).

Filtering
The task is to compute, given the model's parameters and a sequence of observations, the distribution over hidden states of the last latent variable at the end of the sequence, i.e. to compute {\displaystyle P(x(t)\ |\ y(1),\dots ,y(t))} P(x(t)\ |\ y(1),\dots ,y(t)). This task is normally used when the sequence of latent variables is thought of as the underlying states that a process moves through at a sequence of points of time, with corresponding observations at each point in time. Then, it is natural to ask about the state of the process at the end.

This problem can be handled efficiently using the forward algorithm.

Smoothing
This is similar to filtering but asks about the distribution of a latent variable somewhere in the middle of a sequence, i.e. to compute {\displaystyle P(x(k)\ |\ y(1),\dots ,y(t))} P(x(k)\ |\ y(1),\dots ,y(t)) for some {\displaystyle k<t} k<t. From the perspective described above, this can be thought of as the probability distribution over hidden states for a point in time k in the past, relative to time t.

The forward-backward algorithm is an efficient method for computing the smoothed values for all hidden state variables.

Most likely explanation
The task, unlike the previous two, asks about the joint probability of the entire sequence of hidden states that generated a particular sequence of observations (see illustration on the right). This task is generally applicable when HMM's are applied to different sorts of problems from those for which the tasks of filtering and smoothing are applicable. An example is part-of-speech tagging, where the hidden states represent the underlying parts of speech corresponding to an observed sequence of words. In this case, what is of interest is the entire sequence of parts of speech, rather than simply the part of speech for a single word, as filtering or smoothing would compute.

This task requires finding a maximum over all possible state sequences, and can be solved efficiently by the Viterbi algorithm.

Statistical significance
For some of the above problems, it may also be interesting to ask about statistical significance. What is the probability that a sequence drawn from some null distribution will have an HMM probability (in the case of the forward algorithm) or a maximum state sequence probability (in the case of the Viterbi algorithm) at least as large as that of a particular output sequence?[16] When an HMM is used to evaluate the relevance of a hypothesis for a particular output sequence, the statistical significance indicates the false positive rate associated with failing to reject the hypothesis for the output sequence.

A concrete example
Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.

Alice believes that the weather operates as a discrete Markov chain. There are two states, "Rainy" and "Sunny", but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: "walk", "shop", or "clean". Since Bob tells Alice about his activities, those are the observations. The entire system is that of a hidden Markov model (HMM).

Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. They can be represented as follows in Python:

states = ('Rainy', 'Sunny')
 
observations = ('walk', 'shop', 'clean')
 
start_probability = {'Rainy': 0.6, 'Sunny': 0.4}
 
transition_probability = {
   'Rainy' : {'Rainy': 0.7, 'Sunny': 0.3},
   'Sunny' : {'Rainy': 0.4, 'Sunny': 0.6},
   }
 
emission_probability = {
   'Rainy' : {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},
   'Sunny' : {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},
   }
In this piece of code, start_probability represents Alice's belief about which state the HMM is in when Bob first calls her (all she knows is that it tends to be rainy on average). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) approximately {'Rainy': 0.57, 'Sunny': 0.43}. The transition_probability represents the change of the weather in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow will be sunny if today is rainy. The emission_probability represents how likely Bob is to perform a certain activity on each day. If it is rainy, there is a 50% chance that he is cleaning his apartment; if it is sunny, there is a 60% chance that he is outside for a walk.

Graphical representation of the given HMM
A similar example is further elaborated in the Viterbi algorithm page.

Learning
The parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of state transition and emission probabilities. The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the set of output sequences. No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the Baum–Welch algorithm or the Baldi–Chauvin algorithm. The Baum–Welch algorithm is a special case of the expectation-maximization algorithm. If the HMMs are used for time series prediction, more sophisticated Bayesian inference methods, like Markov chain Monte Carlo (MCMC) sampling are proven to be favorable over finding a single maximum likelihood model both in terms of accuracy and stability.[17] Since MCMC imposes significant computational burden, in cases where computational scalability is also of interest, one may alternatively resort to variational approximations to Bayesian inference, e.g.[18] Indeed, approximate variational inference offers computational efficiency comparable to expectation-maximization, while yielding an accuracy profile only slightly inferior to exact MCMC-type Bayesian inference.

Mathematical description
General description
A basic hidden Markov model can be described as follows:

{\displaystyle N} N	{\displaystyle =} =	number of states
{\displaystyle T} T	{\displaystyle =} =	number of observations
{\displaystyle \theta _{i=1\dots N}} \theta _{i=1\dots N}	{\displaystyle =} =	emission parameter for an observation associated with state {\displaystyle i} i
{\displaystyle \phi _{i=1\dots N,j=1\dots N}} \phi _{i=1\dots N,j=1\dots N}	{\displaystyle =} =	probability of transition from state {\displaystyle i} i to state {\displaystyle j} j
{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	{\displaystyle =} =	{\displaystyle N} N-dimensional vector, composed of {\displaystyle \phi _{i,j=1\dots N}} {\displaystyle \phi _{i,j=1\dots N}}; the {\displaystyle i} i-th row of the matrix {\displaystyle \phi _{i=1\dots N,j=1\dots N}} \phi _{i=1\dots N,j=1\dots N} (sum of it is {\displaystyle 1} 1)
{\displaystyle x_{t=1\dots T}} x_{t=1\dots T}	{\displaystyle =} =	(hidden) state at time {\displaystyle t} t
{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	{\displaystyle =} =	observation at time {\displaystyle t} t
{\displaystyle F(y|\theta )} F(y|\theta )	{\displaystyle =} =	probability distribution of an observation, parametrized on {\displaystyle \theta } \theta 
{\displaystyle x_{t=2\dots T}} x_{t=2\dots T}	{\displaystyle \sim } \sim 	{\displaystyle \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})} \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})
{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	{\displaystyle \sim } \sim 	{\displaystyle F(\theta _{x_{t}})} F(\theta _{x_{t}})
Note that, in the above model (and also the one below), the prior distribution of the initial state {\displaystyle x_{1}} x_{1} is not specified. Typical learning models correspond to assuming a discrete uniform distribution over possible states (i.e. no particular prior distribution is assumed).

In a Bayesian setting, all parameters are associated with random variables, as follows:

{\displaystyle N,T} N,T	{\displaystyle =} =	as above
{\displaystyle \theta _{i=1\dots N},\phi _{i=1\dots N,j=1\dots N},{\boldsymbol {\phi }}_{i=1\dots N}} \theta _{i=1\dots N},\phi _{i=1\dots N,j=1\dots N},{\boldsymbol {\phi }}_{i=1\dots N}	{\displaystyle =} =	as above
{\displaystyle x_{t=1\dots T},y_{t=1\dots T},F(y|\theta )} x_{t=1\dots T},y_{t=1\dots T},F(y|\theta )	{\displaystyle =} =	as above
{\displaystyle \alpha } \alpha 	{\displaystyle =} =	shared hyperparameter for emission parameters
{\displaystyle \beta } \beta 	{\displaystyle =} =	shared hyperparameter for transition parameters
{\displaystyle H(\theta |\alpha )} H(\theta |\alpha )	{\displaystyle =} =	prior probability distribution of emission parameters, parametrized on {\displaystyle \alpha } \alpha 
{\displaystyle \theta _{i=1\dots N}} \theta _{i=1\dots N}	{\displaystyle \sim } \sim 	{\displaystyle H(\alpha )} H(\alpha )
{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	{\displaystyle \sim } \sim 	{\displaystyle \operatorname {Symmetric-Dirichlet} _{N}(\beta )} \operatorname {Symmetric-Dirichlet} _{N}(\beta )
{\displaystyle x_{t=2\dots T}} x_{t=2\dots T}	{\displaystyle \sim } \sim 	{\displaystyle \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})} \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})
{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	{\displaystyle \sim } \sim 	{\displaystyle F(\theta _{x_{t}})} F(\theta _{x_{t}})
These characterizations use {\displaystyle F} F and {\displaystyle H} H to describe arbitrary distributions over observations and parameters, respectively. Typically {\displaystyle H} H will be the conjugate prior of {\displaystyle F} F. The two most common choices of {\displaystyle F} F are Gaussian and categorical; see below.

Compared with a simple mixture model
As mentioned above, the distribution of each observation in a hidden Markov model is a mixture density, with the states of the corresponding to mixture components. It is useful to compare the above characterizations for an HMM with the corresponding characterizations, of a mixture model, using the same notation.

A non-Bayesian mixture model:

 	 	{\displaystyle N} N	 	{\displaystyle =} =	 	number of mixture components
 	 	{\displaystyle T} T	 	{\displaystyle =} =	 	number of observations
 	 	{\displaystyle \theta _{i=1\dots N}} \theta _{i=1\dots N}	 	{\displaystyle =} =	 	parameter of distribution of observation associated with component {\displaystyle i} i
 	 	{\displaystyle \phi _{i=1\dots N}} \phi _{i=1\dots N}	 	{\displaystyle =} =	 	mixture weight, i.e. prior probability of component {\displaystyle i} i
 	 	{\displaystyle {\boldsymbol {\phi }}} {\boldsymbol {\phi }}	 	{\displaystyle =} =	 	{\displaystyle N} N-dimensional vector, composed of {\displaystyle \phi _{1\dots N}} \phi _{1\dots N}; must sum to {\displaystyle 1} 1
 	 	{\displaystyle x_{t=1\dots T}} x_{t=1\dots T}	 	{\displaystyle =} =	 	component of observation {\displaystyle t} t
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle =} =	 	observation {\displaystyle t} t
 	 	{\displaystyle F(y|\theta )} F(y|\theta )	 	{\displaystyle =} =	 	probability distribution of an observation, parametrized on {\displaystyle \theta } \theta 
 	 	{\displaystyle x_{t=1\dots T}} x_{t=1\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Categorical} ({\boldsymbol {\phi }})} \operatorname {Categorical} ({\boldsymbol {\phi }})
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle F(\theta _{x_{t}})} F(\theta _{x_{t}})
A Bayesian mixture model:

 	 	{\displaystyle N,T} N,T	 	{\displaystyle =} =	 	as above
 	 	{\displaystyle \theta _{i=1\dots N},\phi _{i=1\dots N},{\boldsymbol {\phi }}} \theta _{i=1\dots N},\phi _{i=1\dots N},{\boldsymbol {\phi }}	 	{\displaystyle =} =	 	as above
 	 	{\displaystyle x_{t=1\dots T},y_{t=1\dots T},F(y|\theta )} x_{t=1\dots T},y_{t=1\dots T},F(y|\theta )	 	{\displaystyle =} =	 	as above
 	 	{\displaystyle \alpha } \alpha 	 	{\displaystyle =} =	 	shared hyperparameter for component parameters
 	 	{\displaystyle \beta } \beta 	 	{\displaystyle =} =	 	shared hyperparameter for mixture weights
 	 	{\displaystyle H(\theta |\alpha )} H(\theta |\alpha )	 	{\displaystyle =} =	 	prior probability distribution of component parameters, parametrized on {\displaystyle \alpha } \alpha 
 	 	{\displaystyle \theta _{i=1\dots N}} \theta _{i=1\dots N}	 	{\displaystyle \sim } \sim 	 	{\displaystyle H(\alpha )} H(\alpha )
 	 	{\displaystyle {\boldsymbol {\phi }}} {\boldsymbol {\phi }}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Symmetric-Dirichlet} _{N}(\beta )} \operatorname {Symmetric-Dirichlet} _{N}(\beta )
 	 	{\displaystyle x_{t=1\dots T}} x_{t=1\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Categorical} ({\boldsymbol {\phi }})} \operatorname {Categorical} ({\boldsymbol {\phi }})
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle F(\theta _{x_{t}})} F(\theta _{x_{t}})
Examples
The following mathematical descriptions are fully written out and explained, for ease of implementation.

A typical non-Bayesian HMM with Gaussian observations looks like this:

 	 	{\displaystyle N} N	 	{\displaystyle =} =	 	number of states
 	 	{\displaystyle T} T	 	{\displaystyle =} =	 	number of observations
 	 	{\displaystyle \phi _{i=1\dots N,j=1\dots N}} \phi _{i=1\dots N,j=1\dots N}	 	{\displaystyle =} =	 	probability of transition from state {\displaystyle i} i to state {\displaystyle j} j
 	 	{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	 	{\displaystyle =} =	 	{\displaystyle N} N-dimensional vector, composed of {\displaystyle \phi _{i,1\dots N}} \phi _{i,1\dots N}; must sum to {\displaystyle 1} 1
 	 	{\displaystyle \mu _{i=1\dots N}} \mu _{i=1\dots N}	 	{\displaystyle =} =	 	mean of observations associated with state {\displaystyle i} i
 	 	{\displaystyle \sigma _{i=1\dots N}^{2}} \sigma _{i=1\dots N}^{2}	 	{\displaystyle =} =	 	variance of observations associated with state {\displaystyle i} i
 	 	{\displaystyle x_{t=1\dots T}} x_{t=1\dots T}	 	{\displaystyle =} =	 	state of observation at time {\displaystyle t} t
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle =} =	 	observation at time {\displaystyle t} t
 	 	{\displaystyle x_{t=2\dots T}} x_{t=2\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})} \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle {\mathcal {N}}(\mu _{x_{t}},\sigma _{x_{t}}^{2})} {\mathcal {N}}(\mu _{x_{t}},\sigma _{x_{t}}^{2})
A typical Bayesian HMM with Gaussian observations looks like this:

 	 	{\displaystyle N} N	 	{\displaystyle =} =	 	number of states
 	 	{\displaystyle T} T	 	{\displaystyle =} =	 	number of observations
 	 	{\displaystyle \phi _{i=1\dots N,j=1\dots N}} \phi _{i=1\dots N,j=1\dots N}	 	{\displaystyle =} =	 	probability of transition from state {\displaystyle i} i to state {\displaystyle j} j
 	 	{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	 	{\displaystyle =} =	 	{\displaystyle N} N-dimensional vector, composed of {\displaystyle \phi _{i,1\dots N}} \phi _{i,1\dots N}; must sum to {\displaystyle 1} 1
 	 	{\displaystyle \mu _{i=1\dots N}} \mu _{i=1\dots N}	 	{\displaystyle =} =	 	mean of observations associated with state {\displaystyle i} i
 	 	{\displaystyle \sigma _{i=1\dots N}^{2}} \sigma _{i=1\dots N}^{2}	 	{\displaystyle =} =	 	variance of observations associated with state {\displaystyle i} i
 	 	{\displaystyle x_{t=1\dots T}} x_{t=1\dots T}	 	{\displaystyle =} =	 	state of observation at time {\displaystyle t} t
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle =} =	 	observation at time {\displaystyle t} t
 	 	{\displaystyle \beta } \beta 	 	{\displaystyle =} =	 	concentration hyperparameter controlling the density of the transition matrix
 	 	{\displaystyle \mu _{0},\lambda } \mu _{0},\lambda 	 	{\displaystyle =} =	 	shared hyperparameters of the means for each state
 	 	{\displaystyle \nu ,\sigma _{0}^{2}} \nu ,\sigma _{0}^{2}	 	{\displaystyle =} =	 	shared hyperparameters of the variances for each state
 	 	{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Symmetric-Dirichlet} _{N}(\beta )} \operatorname {Symmetric-Dirichlet} _{N}(\beta )
 	 	{\displaystyle x_{t=2\dots T}} x_{t=2\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})} \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})
 	 	{\displaystyle \mu _{i=1\dots N}} \mu _{i=1\dots N}	 	{\displaystyle \sim } \sim 	 	{\displaystyle {\mathcal {N}}(\mu _{0},\lambda \sigma _{i}^{2})} {\mathcal {N}}(\mu _{0},\lambda \sigma _{i}^{2})
 	 	{\displaystyle \sigma _{i=1\dots N}^{2}} \sigma _{i=1\dots N}^{2}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Inverse-Gamma} (\nu ,\sigma _{0}^{2})} \operatorname {Inverse-Gamma} (\nu ,\sigma _{0}^{2})
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle {\mathcal {N}}(\mu _{x_{t}},\sigma _{x_{t}}^{2})} {\mathcal {N}}(\mu _{x_{t}},\sigma _{x_{t}}^{2})
A typical non-Bayesian HMM with categorical observations looks like this:

 	 	{\displaystyle N} N	 	{\displaystyle =} =	 	number of states
 	 	{\displaystyle T} T	 	{\displaystyle =} =	 	number of observations
 	 	{\displaystyle \phi _{i=1\dots N,j=1\dots N}} \phi _{i=1\dots N,j=1\dots N}	 	{\displaystyle =} =	 	probability of transition from state {\displaystyle i} i to state {\displaystyle j} j
 	 	{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	 	{\displaystyle =} =	 	{\displaystyle N} N-dimensional vector, composed of {\displaystyle \phi _{i,1\dots N}} \phi _{i,1\dots N}; must sum to {\displaystyle 1} 1
 	 	{\displaystyle V} V	 	{\displaystyle =} =	 	dimension of categorical observations, e.g. size of word vocabulary
 	 	{\displaystyle \theta _{i=1\dots N,j=1\dots V}} \theta _{i=1\dots N,j=1\dots V}	 	{\displaystyle =} =	 	probability for state {\displaystyle i} i of observing the {\displaystyle j} jth item
 	 	{\displaystyle {\boldsymbol {\theta }}_{i=1\dots N}} {\boldsymbol {\theta }}_{i=1\dots N}	 	{\displaystyle =} =	 	{\displaystyle V} V-dimensional vector, composed of {\displaystyle \theta _{i,1\dots V}} \theta _{i,1\dots V}; must sum to {\displaystyle 1} 1
 	 	{\displaystyle x_{t=1\dots T}} x_{t=1\dots T}	 	{\displaystyle =} =	 	state of observation at time {\displaystyle t} t
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle =} =	 	observation at time {\displaystyle t} t
 	 	{\displaystyle x_{t=2\dots T}} x_{t=2\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})} \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle {\text{Categorical}}({\boldsymbol {\theta }}_{x_{t}})} {\text{Categorical}}({\boldsymbol {\theta }}_{x_{t}})
A typical Bayesian HMM with categorical observations looks like this:

 	 	{\displaystyle N} N	 	{\displaystyle =} =	 	number of states
 	 	{\displaystyle T} T	 	{\displaystyle =} =	 	number of observations
 	 	{\displaystyle \phi _{i=1\dots N,j=1\dots N}} \phi _{i=1\dots N,j=1\dots N}	 	{\displaystyle =} =	 	probability of transition from state {\displaystyle i} i to state {\displaystyle j} j
 	 	{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	 	{\displaystyle =} =	 	{\displaystyle N} N-dimensional vector, composed of {\displaystyle \phi _{i,1\dots N}} \phi _{i,1\dots N}; must sum to {\displaystyle 1} 1
 	 	{\displaystyle V} V	 	{\displaystyle =} =	 	dimension of categorical observations, e.g. size of word vocabulary
 	 	{\displaystyle \theta _{i=1\dots N,j=1\dots V}} \theta _{i=1\dots N,j=1\dots V}	 	{\displaystyle =} =	 	probability for state {\displaystyle i} i of observing the {\displaystyle j} jth item
 	 	{\displaystyle {\boldsymbol {\theta }}_{i=1\dots N}} {\boldsymbol {\theta }}_{i=1\dots N}	 	{\displaystyle =} =	 	{\displaystyle V} V-dimensional vector, composed of {\displaystyle \theta _{i,1\dots V}} \theta _{i,1\dots V}; must sum to {\displaystyle 1} 1
 	 	{\displaystyle x_{t=1\dots T}} x_{t=1\dots T}	 	{\displaystyle =} =	 	state of observation at time {\displaystyle t} t
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle =} =	 	observation at time {\displaystyle t} t
 	 	{\displaystyle \alpha } \alpha 	 	{\displaystyle =} =	 	shared concentration hyperparameter of {\displaystyle {\boldsymbol {\theta }}} {\boldsymbol {\theta }} for each state
 	 	{\displaystyle \beta } \beta 	 	{\displaystyle =} =	 	concentration hyperparameter controlling the density of the transition matrix
 	 	{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Symmetric-Dirichlet} _{N}(\beta )} \operatorname {Symmetric-Dirichlet} _{N}(\beta )
 	 	{\displaystyle {\boldsymbol {\theta }}_{1\dots V}} {\boldsymbol {\theta }}_{1\dots V}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Symmetric-Dirichlet} _{V}(\alpha )} \operatorname {Symmetric-Dirichlet} _{V}(\alpha )
 	 	{\displaystyle x_{t=2\dots T}} x_{t=2\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})} \operatorname {Categorical} ({\boldsymbol {\phi }}_{x_{t-1}})
 	 	{\displaystyle y_{t=1\dots T}} y_{t=1\dots T}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Categorical} ({\boldsymbol {\theta }}_{x_{t}})} \operatorname {Categorical} ({\boldsymbol {\theta }}_{x_{t}})
Note that in the above Bayesian characterizations, {\displaystyle \beta } \beta  (a concentration parameter) controls the density of the transition matrix. That is, with a high value of {\displaystyle \beta } \beta  (significantly above 1), the probabilities controlling the transition out of a particular state will all be similar, meaning there will be a significant probability of transitioning to any of the other states. In other words, the path followed by the Markov chain of hidden states will be highly random. With a low value of {\displaystyle \beta } \beta  (significantly below 1), only a small number of the possible transitions out of a given state will have significant probability, meaning that the path followed by the hidden states will be somewhat predictable.

A two-level Bayesian HMM
An alternative for the above two Bayesian examples would be to add another level of prior parameters for the transition matrix. That is, replace the lines

 	 	{\displaystyle \beta } \beta 	 	{\displaystyle =} =	 	concentration hyperparameter controlling the density of the transition matrix
 	 	{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Symmetric-Dirichlet} _{N}(\beta )} \operatorname {Symmetric-Dirichlet} _{N}(\beta )
with the following:

 	 	{\displaystyle \gamma } \gamma 	 	{\displaystyle =} =	 	concentration hyperparameter controlling how many states are intrinsically likely
 	 	{\displaystyle \beta } \beta 	 	{\displaystyle =} =	 	concentration hyperparameter controlling the density of the transition matrix
 	 	{\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }}	 	{\displaystyle =} =	 	{\displaystyle N} N-dimensional vector of probabilities, specifying the intrinsic probability of a given state
 	 	{\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Symmetric-Dirichlet} _{N}(\gamma )} \operatorname {Symmetric-Dirichlet} _{N}(\gamma )
 	 	{\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N}	 	{\displaystyle \sim } \sim 	 	{\displaystyle \operatorname {Dirichlet} _{N}(\beta N{\boldsymbol {\eta }})} \operatorname {Dirichlet} _{N}(\beta N{\boldsymbol {\eta }})
What this means is the following:

{\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }} is a probability distribution over states, specifying which states are inherently likely. The greater the probability of a given state in this vector, the more likely is a transition to that state (regardless of the starting state).
{\displaystyle \gamma } \gamma  controls the density of {\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }}. Values significantly above 1 cause a dense vector where all states will have similar prior probabilities. Values significantly below 1 cause a sparse vector where only a few states are inherently likely (have prior probabilities significantly above 0).
{\displaystyle \beta } \beta  controls the density of the transition matrix, or more specifically, the density of the N different probability vectors {\displaystyle {\boldsymbol {\phi }}_{i=1\dots N}} {\boldsymbol {\phi }}_{i=1\dots N} specifying the probability of transitions out of state i to any other state.
Imagine that the value of {\displaystyle \beta } \beta  is significantly above 1. Then the different {\displaystyle {\boldsymbol {\phi }}} {\boldsymbol {\phi }} vectors will be dense, i.e. the probability mass will be spread out fairly evenly over all states. However, to the extent that this mass is unevenly spread, {\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }} controls which states are likely to get more mass than others.

Now, imagine instead that {\displaystyle \beta } \beta  is significantly below 1. This will make the {\displaystyle {\boldsymbol {\phi }}} {\boldsymbol {\phi }} vectors sparse, i.e. almost all the probability mass is distributed over a small number of states, and for the rest, a transition to that state will be very unlikely. Notice that there are different {\displaystyle {\boldsymbol {\phi }}} {\boldsymbol {\phi }} vectors for each starting state, and so even if all the vectors are sparse, different vectors may distribute the mass to different ending states. However, for all of the vectors, {\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }} controls which ending states are likely to get mass assigned to them. For example, if {\displaystyle \beta } \beta  is 0.1, then each {\displaystyle {\boldsymbol {\phi }}} {\boldsymbol {\phi }} will be sparse and, for any given starting state i, the set of states {\displaystyle \mathbf {J} _{i}} \mathbf {J} _{i} to which transitions are likely to occur will be very small, typically having only one or two members. Now, if the probabilities in {\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }} are all the same (or equivalently, one of the above models without {\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }} is used), then for different i, there will be different states in the corresponding {\displaystyle \mathbf {J} _{i}} \mathbf {J} _{i}, so that all states are equally likely to occur in any given {\displaystyle \mathbf {J} _{i}} \mathbf {J} _{i}. On the other hand, if the values in {\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }} are unbalanced, so that one state has a much higher probability than others, almost all {\displaystyle \mathbf {J} _{i}} \mathbf {J} _{i} will contain this state; hence, regardless of the starting state, transitions will nearly always occur to this given state.

Hence, a two-level model such as just described allows independent control over (1) the overall density of the transition matrix, and (2) the density of states to which transitions are likely (i.e. the density of the prior distribution of states in any particular hidden variable {\displaystyle x_{i}} x_{i}). In both cases this is done while still assuming ignorance over which particular states are more likely than others. If it is desired to inject this information into the model, the probability vector {\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }} can be directly specified; or, if there is less certainty about these relative probabilities, a non-symmetric Dirichlet distribution can be used as the prior distribution over {\displaystyle {\boldsymbol {\eta }}} {\boldsymbol {\eta }}. That is, instead of using a symmetric Dirichlet distribution with the single parameter {\displaystyle \gamma } \gamma  (or equivalently, a general Dirichlet with a vector all of whose values are equal to {\displaystyle \gamma } \gamma ), use a general Dirichlet with values that are variously greater or less than {\displaystyle \gamma } \gamma , according to which state is more or less preferred.

Poisson hidden Markov model
Poisson hidden Markov models (PHMM) are special cases of hidden Markov models where a Poisson process has a rate which varies in association with changes between the different states of a Markov model.[19] PHMMs are not necessarily Markovian processes themselves because the underlying Markov chain or Markov process cannot be observed and only the Poisson signal is observed.

Applications
HMMs can be applied in many fields where the goal is to recover a data sequence that is not immediately observable (but other data that depend on the sequence are). Applications include:

Computational finance[20][21]
Single-molecule kinetic analysis[22]
Cryptanalysis
Speech recognition, including Siri[23]
Speech synthesis
Part-of-speech tagging
Document separation in scanning solutions
Machine translation
Partial discharge
Gene prediction
Handwriting recognition
Alignment of bio-sequences
Time series analysis
Activity recognition
Protein folding[24]
Sequence classification[25]
Metamorphic virus detection[26]
DNA motif discovery[27]
ChromHMM[28]
History
The forward and backward recursions used in HMM as well as computations of marginal smoothing probabilities were first described by Ruslan L. Stratonovich in 1960[6] (pages 160—162) and in the late 1950s in his papers in Russian. The Hidden Markov Models were later described in a series of statistical papers by Leonard E. Baum and other authors in the second half of the 1960s. One of the first applications of HMMs was speech recognition, starting in the mid-1970s.[29][30][31][32]

In the second half of the 1980s, HMMs began to be applied to the analysis of biological sequences,[33] in particular DNA. Since then, they have become ubiquitous in the field of bioinformatics.[34]

Types
Hidden Markov models can model complex Markov processes where the states emit the observations according to some probability distribution. One such example is the Gaussian distribution, in such a Hidden Markov Model the states output are represented by a Gaussian distribution.

Moreover, it could represent even more complex behavior when the output of the states is represented as mixture of two or more Gaussians, in which case the probability of generating an observation is the product of the probability of first selecting one of the Gaussians and the probability of generating that observation from that Gaussian. In cases of modeled data exhibiting artifacts such as outliers and skewness, one may resort to finite mixtures of heavier-tailed elliptical distributions, such as the multivariate Student's-t distribution, or appropriate non-elliptical distributions, such as the multivariate Normal Inverse-Gaussian.[35]

Extensions
In the hidden Markov models considered above, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). Hidden Markov models can also be generalized to allow continuous state spaces. Examples of such models are those where the Markov process over hidden variables is a linear dynamical system, with a linear relationship among related variables and where all hidden and observed variables follow a Gaussian distribution. In simple cases, such as the linear dynamical system just mentioned, exact inference is tractable (in this case, using the Kalman filter); however, in general, exact inference in HMMs with continuous latent variables is infeasible, and approximate methods must be used, such as the extended Kalman filter or the particle filter.

Hidden Markov models are generative models, in which the joint distribution of observations and hidden states, or equivalently both the prior distribution of hidden states (the transition probabilities) and conditional distribution of observations given states (the emission probabilities), is modeled. The above algorithms implicitly assume a uniform prior distribution over the transition probabilities. However, it is also possible to create hidden Markov models with other types of prior distributions. An obvious candidate, given the categorical distribution of the transition probabilities, is the Dirichlet distribution, which is the conjugate prior distribution of the categorical distribution. Typically, a symmetric Dirichlet distribution is chosen, reflecting ignorance about which states are inherently more likely than others. The single parameter of this distribution (termed the concentration parameter) controls the relative density or sparseness of the resulting transition matrix. A choice of 1 yields a uniform distribution. Values greater than 1 produce a dense matrix, in which the transition probabilities between pairs of states are likely to be nearly equal. Values less than 1 result in a sparse matrix in which, for each given source state, only a small number of destination states have non-negligible transition probabilities. It is also possible to use a two-level prior Dirichlet distribution, in which one Dirichlet distribution (the upper distribution) governs the parameters of another Dirichlet distribution (the lower distribution), which in turn governs the transition probabilities. The upper distribution governs the overall distribution of states, determining how likely each state is to occur; its concentration parameter determines the density or sparseness of states. Such a two-level prior distribution, where both concentration parameters are set to produce sparse distributions, might be useful for example in unsupervised part-of-speech tagging, where some parts of speech occur much more commonly than others; learning algorithms that assume a uniform prior distribution generally perform poorly on this task. The parameters of models of this sort, with non-uniform prior distributions, can be learned using Gibbs sampling or extended versions of the expectation-maximization algorithm.

An extension of the previously described hidden Markov models with Dirichlet priors uses a Dirichlet process in place of a Dirichlet distribution. This type of model allows for an unknown and potentially infinite number of states. It is common to use a two-level Dirichlet process, similar to the previously described model with two levels of Dirichlet distributions. Such a model is called a hierarchical Dirichlet process hidden Markov model, or HDP-HMM for short. It was originally described under the name "Infinite Hidden Markov Model"[4] and was further formalized in[5].

A different type of extension uses a discriminative model in place of the generative model of standard HMMs. This type of model directly models the conditional distribution of the hidden states given the observations, rather than modeling the joint distribution. An example of this model is the so-called maximum entropy Markov model (MEMM), which models the conditional distribution of the states using logistic regression (also known as a "maximum entropy model"). The advantage of this type of model is that arbitrary features (i.e. functions) of the observations can be modeled, allowing domain-specific knowledge of the problem at hand to be injected into the model. Models of this sort are not limited to modeling direct dependencies between a hidden state and its associated observation; rather, features of nearby observations, of combinations of the associated observation and nearby observations, or in fact of arbitrary observations at any distance from a given hidden state can be included in the process used to determine the value of a hidden state. Furthermore, there is no need for these features to be statistically independent of each other, as would be the case if such features were used in a generative model. Finally, arbitrary features over pairs of adjacent hidden states can be used rather than simple transition probabilities. The disadvantages of such models are: (1) The types of prior distributions that can be placed on hidden states are severely limited; (2) It is not possible to predict the probability of seeing an arbitrary observation. This second limitation is often not an issue in practice, since many common usages of HMM's do not require such predictive probabilities.

A variant of the previously described discriminative model is the linear-chain conditional random field. This uses an undirected graphical model (aka Markov random field) rather than the directed graphical models of MEMM's and similar models. The advantage of this type of model is that it does not suffer from the so-called label bias problem of MEMM's, and thus may make more accurate predictions. The disadvantage is that training can be slower than for MEMM's.

Yet another variant is the factorial hidden Markov model, which allows for a single observation to be conditioned on the corresponding hidden variables of a set of {\displaystyle K} K independent Markov chains, rather than a single Markov chain. It is equivalent to a single HMM, with {\displaystyle N^{K}} N^{K} states (assuming there are {\displaystyle N} N states for each chain), and therefore, learning in such a model is difficult: for a sequence of length {\displaystyle T} T, a straightforward Viterbi algorithm has complexity {\displaystyle O(N^{2K}\,T)} O(N^{2K}\,T). To find an exact solution, a junction tree algorithm could be used, but it results in an {\displaystyle O(N^{K+1}\,K\,T)} O(N^{K+1}\,K\,T) complexity. In practice, approximate techniques, such as variational approaches, could be used.[36]

All of the above models can be extended to allow for more distant dependencies among hidden states, e.g. allowing for a given state to be dependent on the previous two or three states rather than a single previous state; i.e. the transition probabilities are extended to encompass sets of three or four adjacent states (or in general {\displaystyle K} K adjacent states). The disadvantage of such models is that dynamic-programming algorithms for training them have an {\displaystyle O(N^{K}\,T)} O(N^{K}\,T) running time, for {\displaystyle K} K adjacent states and {\displaystyle T} T total observations (i.e. a length- {\displaystyle T} T Markov chain).

Another recent extension is the triplet Markov model,[37] in which an auxiliary underlying process is added to model some data specificities. Many variants of this model have been proposed. One should also mention the interesting link that has been established between the theory of evidence and the triplet Markov models[11] and which allows to fuse data in Markovian context[12] and to model nonstationary data.[13][14] Note that alternative multi-stream data fusion strategies have also been proposed in the recent literature, e.g.[38]

Finally, a different rationale towards addressing the problem of modeling nonstationary data by means of hidden Markov models was suggested in 2012.[39] It consists in employing a small recurrent neural network (RNN), specifically a reservoir network,[40] to capture the evolution of the temporal dynamics in the observed data. This information, encoded in the form of a high-dimensional vector, is used as a conditioning variable of the HMM state transition probabilities. Under such a setup, we eventually obtain a nonstationary HMM the transition probabilities of which evolve over time in a manner that is inferred from the data itself, as opposed to some unrealistic ad-hoc model of temporal evolution.

See also
Andrey Markov
Baum–Welch algorithm
Bayesian inference
Bayesian programming
Conditional random field
Estimation theory
HHpred / HHsearch free server and software for protein sequence searching
HMMER, a free hidden Markov model program for protein sequence analysis
Hidden Bernoulli model
Hidden semi-Markov model
Hierarchical hidden Markov model
Layered hidden Markov model
Sequential dynamical system
Stochastic context-free grammar
Time Series Analysis
Variable-order Markov model
Viterbi algorithm
References
 Baum, L. E.; Petrie, T. (1966). "Statistical Inference for Probabilistic Functions of Finite State Markov Chains". The Annals of Mathematical Statistics. 37 (6): 1554–1563. doi:10.1214/aoms/1177699147. Retrieved 28 November 2011.
 Baum, L. E.; Eagon, J. A. (1967). "An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology". Bulletin of the American Mathematical Society. 73 (3): 360. doi:10.1090/S0002-9904-1967-11751-8. Zbl 0157.11101.
 Baum, L. E.; Sell, G. R. (1968). "Growth transformations for functions on manifolds". Pacific Journal of Mathematics. 27 (2): 211–227. doi:10.2140/pjm.1968.27.211. Retrieved 28 November 2011.
 Baum, L. E.; Petrie, T.; Soules, G.; Weiss, N. (1970). "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains". The Annals of Mathematical Statistics. 41: 164. doi:10.1214/aoms/1177697196. JSTOR 2239727. MR 0287613. Zbl 0188.49603.
 Baum, L.E. (1972). "An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of a Markov Process". Inequalities. 3: 1–8.
 Stratonovich, R.L. (1960). "Conditional Markov Processes". Theory of Probability and its Applications. 5 (2): 156–178. doi:10.1137/1105015.
 Thad Starner, Alex Pentland. Real-Time American Sign Language Visual Recognition From Video Using Hidden Markov Models. Master's Thesis, MIT, Feb 1995, Program in Media Arts
 B. Pardo and W. Birmingham. Modeling Form for On-line Following of Musical Performances. AAAI-05 Proc., July 2005.
 Satish L, Gururaj BI (April 2003). "Use of hidden Markov models for partial discharge pattern classification". IEEE Transactions on Dielectrics and Electrical Insulation.
 Li, N; Stephens, M (December 2003). "Modeling linkage disequilibrium and identifying recombination hotspots using single-nucleotide polymorphism data". Genetics. 165 (4): 2213–33. PMC 1462870?Freely accessible. PMID 14704198.
 Pieczynski, Wojciech (2007). "Multisensor triplet Markov chains and theory of evidence". International Journal of Approximate Reasoning. 45: 1–16. doi:10.1016/j.ijar.2006.05.001.
 Boudaren et al., M. Y. Boudaren, E. Monfrini, W. Pieczynski, and A. Aissani, Dempster-Shafer fusion of multisensor signals in nonstationary Markovian context, EURASIP Journal on Advances in Signal Processing, No. 134, 2012.
 Lanchantin et al., P. Lanchantin and W. Pieczynski, Unsupervised restoration of hidden non stationary Markov chain using evidential priors, IEEE Trans. on Signal Processing, Vol. 53, No. 8, pp. 3091-3098, 2005.
 Boudaren et al., M. Y. Boudaren, E. Monfrini, and W. Pieczynski, Unsupervised segmentation of random discrete data hidden with switching noise distributions, IEEE Signal Processing Letters, Vol. 19, No. 10, pp. 619-622, October 2012.
 Lawrence R. Rabiner (February 1989). "A tutorial on Hidden Markov Models and selected applications in speech recognition" (PDF). Proceedings of the IEEE. 77 (2): 257–286. doi:10.1109/5.18626. [1]
 Newberg, L. (2009). "Error statistics of hidden Markov model and hidden Boltzmann model results". BMC Bioinformatics. 10: 212. doi:10.1186/1471-2105-10-212. PMC 2722652?Freely accessible. PMID 19589158. open access publication – free to read
 Sipos, I. Róbert. Parallel stratified MCMC sampling of AR-HMMs for stochastic time series prediction. In: Proceedings, 4th Stochastic Modeling Techniques and Data Analysis International Conference with Demographics Workshop (SMTDA2016), pp. 295-306. Valletta, 2016. PDF
 Chatzis, Sotirios P.; Kosmopoulos, Dimitrios I. (2011). "A variational Bayesian methodology for hidden Markov models utilizing Student's-t mixtures" (PDF). Pattern Recognition. 44 (2): 295–306. CiteSeerX 10.1.1.629.6275?Freely accessible. doi:10.1016/j.patcog.2010.09.001.
 R. Paroli. et al., Poisson hidden Markov models for time series of overdispersed insurance counts
 Sipos, I. Róbert; Ceffer, Attila; Levendovszky, János (2016). "Parallel Optimization of Sparse Portfolios with AR-HMMs". Computational Economics. 49 (4): 563–578. doi:10.1007/s10614-016-9579-y.
 Petropoulos, Anastasios; Chatzis, Sotirios P.; Xanthopoulos, Stylianos (2016). "A novel corporate credit rating system based on Student's-t hidden Markov models". Expert Systems with Applications. 53: 87–105. doi:10.1016/j.eswa.2016.01.015.
 NICOLAI, CHRISTOPHER (2013). "SOLVING ION CHANNEL KINETICS WITH THE QuB SOFTWARE". Biophysical Reviews and Letters. 8 (3n04): 191–211. doi:10.1142/S1793048013300053.
 Domingos, Pedro (2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. p. 37. ISBN 9780465061921.
 Stigler, J.; Ziegler, F.; Gieseke, A.; Gebhardt, J. C. M.; Rief, M. (2011). "The Complex Folding Network of Single Calmodulin Molecules". Science. 334 (6055): 512–516. Bibcode:2011Sci...334..512S. doi:10.1126/science.1207598. PMID 22034433.
 Blasiak, S.; Rangwala, H. (2011). "A Hidden Markov Model Variant for Sequence Classification". IJCAI Proceedings-International Joint Conference on Artificial Intelligence. 22: 1192.
 Wong, W.; Stamp, M. (2006). "Hunting for metamorphic engines". Journal in Computer Virology. 2 (3): 211–229. doi:10.1007/s11416-006-0028-7.
 Wong, K. -C.; Chan, T. -M.; Peng, C.; Li, Y.; Zhang, Z. (2013). "DNA motif elucidation using belief propagation". Nucleic Acids Research. 41 (16): e153. doi:10.1093/nar/gkt574. PMC 3763557?Freely accessible. PMID 23814189.
 "ChromHMM: Chromatin state discovery and characterization". compbio.mit.edu. Retrieved 2018-08-01. line feed character in |title= at position 40 (help)
 Baker, J. (1975). "The DRAGON system—An overview". IEEE Transactions on Acoustics, Speech, and Signal Processing. 23: 24–29. doi:10.1109/TASSP.1975.1162650.
 Jelinek, F.; Bahl, L.; Mercer, R. (1975). "Design of a linguistic statistical decoder for the recognition of continuous speech". IEEE Transactions on Information Theory. 21 (3): 250. doi:10.1109/TIT.1975.1055384.
 Xuedong Huang; M. Jack; Y. Ariki (1990). Hidden Markov Models for Speech Recognition. Edinburgh University Press. ISBN 0-7486-0162-7.
 Xuedong Huang; Alex Acero; Hsiao-Wuen Hon (2001). Spoken Language Processing. Prentice Hall. ISBN 0-13-022616-5.
 M. Bishop and E. Thompson (1986). "Maximum Likelihood Alignment of DNA Sequences". Journal of Molecular Biology. 190 (2): 159–165. doi:10.1016/0022-2836(86)90289-5. PMID 3641921. (subscription required) closed access publication – behind paywall
 Durbin, Richard M.; Eddy, Sean R.; Krogh, Anders; Mitchison, Graeme (1998), Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids (1st ed.), Cambridge, New York: Cambridge University Press, doi:10.2277/0521629713, ISBN 0-521-62971-3, OCLC 593254083
 Sotirios P. Chatzis, "Hidden Markov Models with Nonelliptically Contoured State Densities," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 12, pp. 2297-2304, Dec. 2010. [2]
 Ghahramani, Zoubin; Jordan, Michael I. (1997). "Factorial Hidden Markov Models". Machine Learning. 29 (2/3): 245–273. doi:10.1023/A:1007425814087.
 Pieczynski, Wojciech (2002). "Chai^nes de Markov Triplet". Comptes Rendus Mathématique. 335 (3): 275–278. doi:10.1016/S1631-073X(02)02462-7.
 Sotirios P. Chatzis, Dimitrios Kosmopoulos, "Visual Workflow Recognition Using a Variational Bayesian Treatment of Multistream Fused Hidden Markov Models," IEEE Transactions on Circuits and Systems for Video Technology, vol. 22, no. 7, pp. 1076-1086, July 2012. [3]
 Chatzis, Sotirios P.; Demiris, Yiannis (2012). "A Reservoir-Driven Non-Stationary Hidden Markov Model". Pattern Recognition. 45 (11): 3985–3996. doi:10.1016/j.patcog.2012.04.018.
 M. Lukosevicius, H. Jaeger (2009) Reservoir computing approaches to recurrent neural network training, Computer Science Review 3: 127–149.
External links
	Wikimedia Commons has media related to Hidden Markov Model.
Concepts
Teif, V. B.; Rippe, K. (2010). "Statistical–mechanical lattice models for protein–DNA binding in chromatin". J. Phys.: Condens. Matter. 22 (41): 414105. arXiv:1004.5514?Freely accessible. Bibcode:2010JPCM...22O4105T. doi:10.1088/0953-8984/22/41/414105.
A Revealing Introduction to Hidden Markov Models by Mark Stamp, San Jose State University.
Fitting HMM's with expectation-maximization – complete derivation
A step-by-step tutorial on HMMs (University of Leeds)
Hidden Markov Models (an exposition using basic mathematics)
Hidden Markov Models (by Narada Warakagoda)
Hidden Markov Models: Fundamentals and Applications Part 1, Part 2 (by V. Petrushin)
Lecture on a Spreadsheet by Jason Eisner, Video and interactive spreadsheet
Software
Hidden Markov Model (HMM) Toolbox for Matlab (by Kevin Murphy)
Hidden Markov Model Toolkit (HTK) (a portable toolkit for building and manipulating hidden Markov models)
Hidden Markov Model R-Package to set up, apply and make inference with discrete time and discrete space Hidden Markov Models
zipHMMlib (a library for general (discrete) hidden Markov models, exploiting repetitions in the input sequence to greatly speed up the forward algorithm. Implementation of the posterior decoding algorithm and the Viterbi algorithm are also provided.)
GHMM Library (home page of the GHMM Library project)
Jahmm Java Library (general-purpose Java library)
HMM and other statistical programs (Implementation in C by Tapas Kanungo)
The hmm package A Haskell library for working with Hidden Markov Models.
GT2K Georgia Tech Gesture Toolkit (referred to as GT2K)
Hidden Markov Models -online calculator for HMM – Viterbi path and probabilities. Examples with perl source code.
A discrete Hidden Markov Model class, based on OpenCV.
depmixS4 R-Package (Hidden Markov Models of GLMs and Other Distributions in S4 )
MLPACK contains a C++ implementation of HMMs
Hidden Markov Models Java Library contains basic HMMs abstractions in Java 8
SFIHMM high-speed C code for the estimation of Hidden Markov Models, Viterbi Path Reconstruction, and the generation of simulated data from HMMs.
vte
Stochastic processes
Discrete time	
Bernoulli process Branching process Chinese restaurant process Galton–Watson process Independent and identically distributed random variables Markov chain Moran process Random walk Loop-erased Self-avoiding Biased Maximal entropy
Continuous time	
Bessel process Birth–death process Brownian motion Bridge Excursion Fractional Geometric Meander Cauchy process Contact process Continuous-time random walk Cox process Diffusion process Empirical process Feller process Fleming–Viot process Gamma process Hunt process Interacting particle systems Itô diffusion Itô process Jump diffusion Jump process Lévy process Local time Markov additive process McKean–Vlasov process Ornstein–Uhlenbeck process Poisson process Compound Non-homogeneous Point process Schramm–Loewner evolution Semimartingale Sigma-martingale Stable process Superprocess Telegraph process Variance gamma process Wiener process Wiener sausage
Both	
Branching process Galves–Löcherbach model Gaussian process Hidden Markov model (HMM) Markov process Martingale Differences Local Sub- Super- Random dynamical system Regenerative process Renewal process Stochastic chains with memory of variable length White noise
Fields and other	
Dirichlet process Gaussian random field Gibbs measure Hopfield model Ising model Potts model Boolean network Markov random field Percolation Pitman–Yor process Point process Cox Poisson Random field Random graph
Time series models	
Autoregressive conditional heteroskedasticity (ARCH) model Autoregressive integrated moving average (ARIMA) model Autoregressive (AR) model Autoregressive–moving-average (ARMA) model Generalized autoregressive conditional heteroskedasticity (GARCH) model Moving-average (MA) model
Financial models	
Black–Derman–Toy Black–Karasinski Black–Scholes Chen Constant elasticity of variance (CEV) Cox–Ingersoll–Ross (CIR) Garman–Kohlhagen Heath–Jarrow–Morton (HJM) Heston Ho–Lee Hull–White LIBOR market Rendleman–Bartter SABR volatility Vašícek Wilkie
Actuarial models	
Bühlmann Cramér–Lundberg Risk process Sparre–Anderson
Queueing models	
Bulk Fluid Generalized queueing network M/G/1 M/M/1 M/M/c
Properties	
Càdlàg paths Continuous Continuous paths Ergodic Exchangeable Feller-continuous Gauss–Markov Markov Mixing Piecewise deterministic Predictable Progressively measurable Self-similar Stationary Time-reversible
Limit theorems	
Central limit theorem Donsker's theorem Doob's martingale convergence theorems Ergodic theorem Fisher–Tippett–Gnedenko theorem Large deviation principle Law of large numbers (weak/strong) Law of the iterated logarithm Maximal ergodic theorem Sanov's theorem
Inequalities	
Burkholder–Davis–Gundy Doob's martingale Kunita–Watanabe
Tools	
Cameron–Martin formula Convergence of random variables Doléans-Dade exponential Doob decomposition theorem Doob–Meyer decomposition theorem Doob's optional stopping theorem Dynkin's formula Feynman–Kac formula Filtration Girsanov theorem Infinitesimal generator Itô integral Itô's lemma Kolmogorov continuity theorem Kolmogorov extension theorem Lévy–Prokhorov metric Malliavin calculus Martingale representation theorem Optional stopping theorem Prokhorov's theorem Quadratic variation Reflection principle Skorokhod integral Skorokhod's representation theorem Skorokhod space Snell envelope Stochastic differential equation Tanaka Stopping time Stratonovich integral Uniform integrability Usual hypotheses Wiener space Classical Abstract
Disciplines	
Actuarial mathematics Econometrics Ergodic theory Extreme value theory (EVT) Large deviations theory Mathematical finance Mathematical statistics Probability theory Queueing theory Renewal theory Ruin theory Statistics Stochastic analysis Time series analysis Machine learning
List of topics Category
Authority control Edit this at Wikidata	
GND: 4352479-5
Categories: BioinformaticsHidden Markov modelsMarkov models
Navigation menu
Not logged inTalkContributionsCreate accountLog inArticleTalkReadEditView historySearch

Search Wikipedia
Main page
Contents
Featured content
Current events
Random article
Donate to Wikipedia
Wikipedia store
Interaction
Help
About Wikipedia
Community portal
Recent changes
Contact page
Tools
What links here
Related changes
Upload file
Special pages
Permanent link
Page information
Wikidata item
Cite this page
Print/export
Create a book
Download as PDF
Printable version
In other projects
Wikimedia Commons

Languages
Deutsch
Español
Français
???
Bahasa Indonesia
Italiano
???????
Ti?ng Vi?t
??
16 more
Edit links
This page was last edited on 1 August 2018, at 14:09 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaDevelopersCookie statementMobile viewWikimedia Foundation Powered by MediaWiki
